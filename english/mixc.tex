\documentclass[12pt,russian]{article}
\renewcommand{\baselinestretch}{1}
%\renewcommand{\baselinestretch}{1.5}
\textwidth=158mm
\textheight=232mm
\voffset=-24mm
%\pagestyle{empty}

\usepackage[russian]{babel}
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{multirow}
\newcommand{\Real}{\mathbb R}
\newcommand{\Nat}{\mathbb N}
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\bignorm}[1]{\bigl\Vert#1\bigr\Vert}
\newcommand{\bigabs}[1]{\bigl\vert#1\bigr\vert}
\newcommand{\set}[1]{\left\{#1\right\}}
\renewcommand{\phi}{\varphi}
\newcommand{\eps}{\varepsilon}
\renewcommand{\ge}{\geqslant}
\renewcommand{\le}{\leqslant}
\renewcommand{\liminf}{\underline{\lim}}
\newcommand{\grad}{\triangledown}
\newcommand{\card}{{\rm card}}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lm}{Lemma}
\newtheorem{rem}{Remark}
\newtheorem{cor}{Corollary}
\newcommand{\To}{\longrightarrow}
\newcommand{\Wf}{\stackrel{o\ }{W_1^1}}
\newcommand{\W}{W_1^1}
\newcommand{\sign}{\mathop{\rm sign}\nolimits}
\newcommand{\dist}{\mathop{\rm dist}\nolimits}
\newcounter{pictureCounter}
\begin{document}

\title{On monotonicity of some functionals \\ under rearrangements}
\author{
S.~Bankevich\\
\texttt{Sergey.Bankevich@gmail.com}
\and
A.~Nazarov\\
\texttt{al.il.nazarov@gmail.com}
}

\maketitle

\section{Introduction}

First, we recall the layer cake representation for a measurable function $u: [-1, 1] \to \Real_+$
(here and elsewhere $\Real_+ = [0,\infty)$).
Namely, if we set $\mathcal{A}_t: = \{x \in [-1,1]:\ u(x)> t \}$
then $u(x) = \int_0^\infty \chi_{\mathcal{A}_t}dt$.

We define the monotone rearrangement of a measurable set $E \subset [-1, 1]$ and the
monotone rearrangement of non-negative function $u \in \W (-1, 1)$ as follows:
\begin{eqnarray*}
E^*: = [1 - \abs{E}, 1]; \qquad
u^*(x): = \int_0^\infty \chi_{\mathcal{A}_t^*}dt.
\end{eqnarray*}

Under the same conditions we define the symmetric rearrangement 
(symmetrization) for sets and functions:
\begin{eqnarray*}
\overline{E} := [-\frac{\abs{E}}{2}, \frac{\abs{E}}{2}]; \qquad
\overline{u}(x) := \int_0^\infty \chi_{\overline{\mathcal{A}_t}}dt.
\end{eqnarray*}

We denote by $\mathfrak{F}$ the set of continuous functions 
$F: \Real_+ \times \Real_+ \to \Real_+,$
which are convex and increasing with respect to the second argument.

Let us consider a functional
\begin{equation}
\label{functional}
I(\mathfrak a, u) = \int_{-1}^1 F(u(x), \mathfrak a(x, u(x)) \abs{u'(x)}) dx,
\end{equation}
where $\mathfrak a: [-1, 1] \times \Real_+ \to \Real_+$ is a continuous function, $F \in \mathfrak{F}$.

It is well known that if $\mathfrak a \equiv const$ then the inequalities
\begin{eqnarray}
\label{toprove}
I(\mathfrak a, u^*) & \le & I(\mathfrak a, u), \qquad \qquad u \in \W(-1, 1);\\
\label{toproveSymm}
I(\mathfrak a, \overline{u}) & \le & I(\mathfrak a, u), \qquad \qquad u \in \Wf(-1, 1)
\end{eqnarray}
hold, see for example \cite{Kawohl} and references therein.

The inequality (\ref{toproveSymm}) and its multi-dimensional analogue
are proved in \cite{Br} provided that the function $\mathfrak a$ is even and convex 
with respect to $x$. However, the proof contains a gap,
and in fact this inequality was proved in \cite{Br} only for Lipschitz functions $u$.

Namely, while proving the inequality (\ref{toproveSymm}) for a natural class of functions,
the author of \cite{Br} approximates $u \in \Wf$ having finite integral (\ref{functional})
using piecewise linear functions $u_k$ and claims that $I(\mathfrak a, u_k) \to I(\mathfrak a, u)$.
However, this assertion is not justified and in general is not true.
In 1926, M.~A.~Lavrentiev proposed the first example of an integral functional
for which the infimum over the domain is strictly less than the infimum over the set of 
Lipschitz functions.
Historical overview and simple examples of ``one-dimensional'' 
functionals for which the Lavrentiev phenomenon takes place can be found in \cite{BGH} f.e..
Note that a deep investigation of the Lavrentiev phenomenon for some classes of multidimensional 
functionals was conducted by V.~V.~Zhikov (see, e.g., \cite{Zh1}, \cite{Zh2}).

In the paper \cite{ASC} the absence of the Lavrentiev phenomenon was proved for the functionals
$I(\mathfrak a, u) = \int_{-1}^1 F(u, u')$. Moreover it was shown that for every $u \in \W(-1, 1)$ 
there exists a sequence of Lipschitz functions $u_k$, such that
\begin{equation}
\label{convergence}
u_k \to u \text{ in } \W (-1, 1) \quad \text{ and } \quad I (\mathfrak a, u_k) \to I (\mathfrak a, u).
\end{equation}

We modify the proof from \cite{ASC} and prove the absence of the Lavrentiev phenomenon
for the functionals of the form (\ref{functional}).
This allows us fill a gap in the proof from \cite{Br} in one-dimensional case.
In addition we prove that evenness and convexity of the weight is a necessary condition
for the inequality (\ref{toproveSymm}) to hold.

The bulk of our paper is devoted to the inequality (\ref{toprove}).
We find a necessary and sufficient condition on the weight $\mathfrak a$ for the inequality 
(\ref{toprove}) to hold%
\footnote{In particular, the inequality is satisfied if the weight function $\mathfrak a$ is even 
and concave in $x$.}.
Under certain additional assumptions this result was announced in \cite{DAN}.

We note also that the inequality (\ref{toprove}) was considered in \cite{Lan}
for functionals similar to (\ref{functional}) under additional constraint $u(-1) = 0$.
We obtain necessary and sufficient conditions for (\ref{toprove}) under this constraint.
(The author of \cite{Lan} assumed that the weight $\mathfrak a$ was decreasing in $x$.)

The article is divided into 8 sections.
In section 2 we deduce the assumptions on the weight function $\mathfrak a$ which are necessary for the 
inequality (\ref{toprove}).
Auxiliary statements for weights satisfying necessary conditions are established in section 3.
In section 4 the inequality (\ref{toprove}) is proved for piecewise linear functions $u$.
In section 5 we present the scheme for proving inequality (\ref{toprove}) for a wider class of 
functions $u$.
In section 6 we prove inequality (\ref{toprove}), provided that the weight $\mathfrak a$ first increases, 
then decreases.
Section 7 is devoted to the proof of (\ref{toprove}) under necessary conditions only.
Finally, in the section 8 we deal with symmetric rearrangement.
There we obtain necessary conditions on the weight and complete the proof of (\ref{toproveSymm}).

\section{ The conditions necessary for the inequality (\ref{toprove})}

\begin{thm}
{\bf 1}. Let the inequality (\ref{toprove}) hold for some $F \in \mathfrak {F}$
and arbitrary piecewise linear $u$. Then the weight function $a$ is even with respect to the first argument,
that is $a(x, v) \equiv a(-x, v)$.

{\bf 2}. Let the inequality (\ref{toprove}) hold for arbitrary $F \in \mathfrak{F}$
and arbitrary piecewise linear $u$. Then the weight function $a$ satisfies
\begin{equation}
\label{almostConcave}
a(s, v) + a(t, v) \ge a(1 - t + s, v), \qquad -1 \le s \le t \le 1, v \in \Real_+.
\end{equation}
\end{thm}

\begin{proof}
{\bf 1.} Suppose that $a(x, v) \not \equiv a(-x, v)$.
Then there are $\bar{x} \in (-1, 1 )$ and $\bar{v} \in \Real_+$ such that
$$a(\bar{x}, \bar{v}) < a(-\bar{x}, \bar{v}).$$
Therefore, there is $\eps> 0$ such that
$$\bar{x} - \eps \le x \le \bar{x}, \bar{v} \le v \le \bar{v} + \eps \Rightarrow a(x, v) < a(-x, v).$$
Now we introduce the following function:
$$
\left\{     
\begin{aligned}
u(x) &= \bar{v} + \eps, & x \in [-1,\bar{x}-\eps]\\
u(x) &= \bar{v} + \bar{x} - x, & x \in (\bar{x} - \eps, \bar{x})\\
u(x) &= \bar{v}, & x \in [\bar{x}, 1]
\end{aligned}
\right.
$$
Then $u^*(x, v) = u(-x, v)$ and
\begin{multline*}
I(a, u)-I(a, u^*) = \int_{\bar{x}-\eps}^{\bar{x}} F( \bar{v} + \bar{x} - x, a(x, \bar{v} + \bar{x} - x) ) dx -
\int_{-\bar{x}}^{-\bar{x}+\eps} F( \bar{v} + \bar{x} + x, a(x, \bar{v} + \bar{x} + x) ) dx \\ =
\int_{\bar{x}-\eps}^{\bar{x}} ( F( \bar{v} + \bar{x} - x, a(x, \bar{v} + \bar{x} - x) ) -
F( \bar{v} + \bar{x} - x, a(-x, \bar{v} + \bar{x} - x) ) ) dx < 0,
\end{multline*}
which contradicts the assumption. Thus, the first statement is proved.

{\bf 2.} Suppose that the assumption (\ref{almostConcave}) is not satisfied.
Then, by continuity of $a$, there exist $-1 \le s \le t \le 1$, $\eps, \delta> 0$ and $\bar{v} \in \Real_+$, such that
for any $0 \le y \le \eps$ and $\bar{v} \le v \le \bar{v} + \eps$ the following inequality holds:
$$a(s + y, v) + a(t - y, v) + \delta < a( 1 - t + s + 2y, v).$$

Consider the function $u$ (see fig. \ref{uGraph}):
\begin{equation}
\label{parLinU}
\left\{     
\begin{aligned}
u(x) &= \bar{v}, & x \in [-1, s] \cup [t, 1]\\
u(x) &= \bar{v} + x - s, & x \in [s, s + \eps]\\
u(x) &= \bar{v} + \eps, & x \in [s + \eps, t - \eps]\\
u(x) &= \bar{v} + t - x, & x \in [t - \eps, t]
\end{aligned}
\right.
\end{equation}

\begin{center}
\begin{picture}(200,90)
\refstepcounter{pictureCounter}
\label{uGraph}
\put(10,65){\line(1,0){50}}
\put(60,65){\line(1,1){10}}
\put(70,75){\line(1,0){40}}
\put(110,75){\line(1,-1){10}}
\put(120,65){\line(1,0){70}}
\put(0,25){\vector(1,0){200}}
\put(100,15){\vector(0,1){80}}
\put(99,65){\line(1,0){2}}
\put(92,62){$\bar{v}$}
\put(60,24){\line(0,1){2}}
\put(58,14){$s$}
\put(120,24){\line(0,1){2}}
\put(119,14){$t$}
\put(10,24){\line(0,1){2}}
\put(6,14){$-1$}
\put(190,24){\line(0,1){2}}
\put(188,14){$1$}
\put(20,70){$u(x)$}
\put(85,1){Fig. \arabic{pictureCounter}}
\end{picture}
\end{center}
Then
$$
\left\{     
\begin{aligned}
u^*(x) &= \bar{v}, & x \in [-1, 1 - t + s]\\
u^*(x) &= \bar{v} + x - s, & x \in [s, s + \eps]\\
u^*(x) &= \bar{v} + \frac{ x - ( 1 - t + s ) }{2}, & x \in [1 - t + s, 1 - t + s + 2\eps]
\end{aligned}
\right.
$$
(see fig. \ref{uStarGraph}).

\begin{center}
\begin{picture}(200,90)
\refstepcounter{pictureCounter}
\label{uStarGraph}
\put(10,65){\line(1,0){120}}
\put(130,64){\line(2,1){20}}
\put(150,75){\line(1,0){40}}
\put(0,25){\vector(1,0){200}}
\put(100,15){\vector(0,1){80}}
\put(99,65){\line(1,0){2}}
\put(92,67){$\bar{v}$}
\put(130,24){\line(0,1){2}}
\put(110,14){$1 - t + s$}
\put(10,24){\line(0,1){2}}
\put(6,14){$-1$}
\put(190,24){\line(0,1){2}}
\put(188,14){$1$}
\put(20,70){$u^*(x)$}
\put(85,1){Fig. \arabic{pictureCounter}}
\end{picture}
\end{center}

We have
\begin{multline*}
I(\mathfrak a, u^*) = \int_0^{2\eps} F(u(1 - t + s + z), \frac{\mathfrak a(1 - t + s + z, u(1 - t + s + z))}{2}) dz\\
= \int_0^\eps 2 F(\bar{v} + y, \frac{\mathfrak a(1 - t + s + 2y, \bar{v} + y)}{2}) dy\\
0 \le I( \mathfrak a, u ) - I( \mathfrak a, u^* ) =
\int_0^\eps ( F(\bar{v} + y, \mathfrak a(s + y, \bar{v} + y)) + F(\bar{v} + y, \mathfrak a( t - y, \bar{v} + y))\\
- 2 F(\bar{v} + y, \frac{ \mathfrak a(1 - t + s + 2y, \bar{v} + y) }{2})) dy\\
< \int_0^\eps ( F(\bar{v} + y, \mathfrak a(s + y, \bar{v} + y)) + F(\bar{v} + y, \mathfrak a(t - y, \bar{v} + y))\\
- 2 F(\bar{v} + y, \frac{ \mathfrak a(s + y, \bar{v} + y) + \mathfrak a(t - y, \bar{v} + y) + \delta }{2})) dy =: J.
\end{multline*}

Let us consider the function $F(v, p) = p ^ \alpha$.
For $\alpha = 1$, the following inequality trivially holds:
\begin{equation}
\label{anticonvex}
\frac{F(v, p) + F(v, q)}{ 2 } - F(v, \frac{p + q}{ 2 } + \frac{\delta}{ 2}) <0.
\end{equation}
We are interested in $p, q$, lying on a compact $[0 , A]$,
where 
$$A=\max \limits_{(x, v)} \mathfrak a,\qquad (x, v) \in [-1, 1 ] \times u([-1, 1] ).
$$
Therefore, there is an $\alpha> 1$, for which the inequality (\ref{anticonvex})
still holds.
For example, any $1 < \alpha < (\log_2 \frac{ 2 A}{A + \delta})^{-1}$ is suitable.

Thus, we obtain a function $F$ strictly convex with respect to the second argument
for which $J \le 0$. This contradiction proves the second statement.
\end{proof}

\begin{rem}
\label{landesNecessary}
It can be seen that proving the second statement of the theorem
one can replace the function $u$ on the interval $[-1, s]$ by any increasing function.
Thus, in the case where $u$ is pinned at the left end {\rm ($u(-1) = 0$)}
the assumption (\ref{almostConcave}) is also necessary for the inequality (\ref{toprove}) to hold.
\end{rem}

\begin{rem}
If a  non-negative function $\mathfrak a$ is even and concave with respect to the first argument then it satisfies the assumption (\ref{almostConcave}). 
Indeed, for every $s$, $t$ and $u$ we have $\mathfrak a( 1 , u) - \mathfrak a(s, u) \le \mathfrak a(t, u) - \mathfrak a(-1 + t - s, u)$.
Since $\mathfrak a(1 , u) \ge 0$, we obtain $\mathfrak a(s, u) + \mathfrak a(t, u) \ge \mathfrak a(-1 + t - s, u) = \mathfrak a( 1 - t + s, u)$.
The converse is not true in general, that is not any even
non-negative function satisfying (\ref{almostConcave}) is concave.
\end{rem}

\section{Properties of the weighting function }

For brevity, in this section we omit the second argument of the function $a$.

\begin{lm}
\label{weightSum}
Consider a continuous function $\mathfrak a \ge 0$ defined on $[-1, 1]$
and satisfying the condition (\ref{almostConcave}).
Then for any $-1 \le t_1 \le t_2 \le \ldots \le t_n \le 1$
the following inequalities hold
\begin{align*}
\sum_{k = 1}^n \mathfrak a(t_k) & \ge \mathfrak a( 1 - \sum_{k = 1}^n (-1)^k t_k), & \text{ for even $n$}, & \\
\sum_{k = 1}^n \mathfrak a(t_k) & \ge \mathfrak a(- \sum_{k = 1}^n (-1)^k t_k), & \text{ for odd $n$}. &
\end{align*}
\end{lm}

\begin{proof}
We will prove the lemma by induction.
For $n = 1$ the assertion is trivial.
Now let $n$ be even. Then, by the induction hypothesis,
$$\sum_{k=1}^{n - 1} \mathfrak a(t_k) \ge \mathfrak a( -\sum_{k = 1}^{n - 1} (-1)^k t_k ).$$
Then
$$\sum_{k = 1}^{n - 1} \mathfrak a( t_k ) + \mathfrak a( t_n ) \ge \mathfrak a( -\sum_{k = 1}^{n - 1} (-1)^k t_k ) + \mathfrak a( t_n ) \ge
\mathfrak a( 1 - \sum_{k = 1}^{n} (-1)^k t_k ).$$
In the case of odd $n$ we have the following induction hypothesis:
$$\sum_{k=2}^n \mathfrak a(t_k) \ge \mathfrak a( 1 + \sum_{k = 2}^n (-1)^k t_k ).$$
Then
$$\mathfrak a( t_1 ) + \sum_{k = 2}^n \mathfrak a( t_k ) \ge \mathfrak a( t_1 ) + \mathfrak a( 1 + \sum_{k = 2}^{n} (-1)^k t_k ) \ge
\mathfrak a( -\sum_{k = 2}^{n} (-1)^k t_k + t_1 ) = \mathfrak a( -\sum_{k = 1}^{n} (-1)^k t_k ).$$
\end{proof}

\begin{rem}
Assume that in addition to the assumptions of Lemma \ref{weightSum} the function $\mathfrak a$ is even.
Then the following inequalities also hold:
\label{almostConcaveMultRem}
\begin{align*}
\sum_{k = 1}^n \mathfrak a(t_k) & \ge \mathfrak a(-1 + \sum_{k = 1}^n (-1)^k t_k), & \text{ for even $n$}, & \\
\sum_{k = 1}^n \mathfrak a(t_k) & \ge \mathfrak a(\sum_{k = 1}^n (-1)^k t_k), & \text{ for odd $n$}. &
\end{align*}
\end{rem}

\begin{lm}
\label{periodicity}
{\bf 1.} Let $\mathfrak a$ satisfy $(\ref{almostConcave})$.
If there is $x_0 \in [-1, 1]$, such that $\mathfrak a(x_0) = 0$,
then either $\mathfrak a\Big |_{[x_0, 1]} \equiv 0$
or the set of zeros of $\mathfrak a$ is periodic on $[x_0, 1]$
and the period is a divisor of $1 - x_0$.

{\bf 2.} Let $\mathfrak a$ be even and satisfy $(\ref{almostConcave})$.
If there is $x_0 \in [-1, 1]$, such that $\mathfrak a(x_0) = 0$,
then either $\mathfrak a \equiv 0$
or the function $\mathfrak a$ is periodic on the $[-1, 1]$ interval
and the period is a divisor of $1 - x_0$.
\end{lm}

\begin{proof}
{\bf 1.}
Note that if $\mathfrak a(s) = \mathfrak a(t) = 0$ holds for some $s \le t$
then the inequality (\ref{almostConcave}) implies
$$0 = \mathfrak a(s) + \mathfrak a(t) \ge \mathfrak a( 1 - (t - s) ) \ge 0$$
i.e. $\mathfrak a(1 - (t - s)) = 0$.
Substituting $s = t = x_0$, we obtain $\mathfrak a(1) = 0$.

Similarly, if $s \le 1 - t$ and $\mathfrak a(s) = \mathfrak a(1 - t) = 0$, then $\mathfrak a(s + t) = 0$.

Thus, the set of roots of $\mathfrak a$ is symmetric and
whenever $s$ and $s + \Delta$ ($\Delta \ge 0$) are roots of $\mathfrak a$,
values $s + k\Delta$ are roots of $\mathfrak a$ too provided $s + k\Delta \le 1$.
This implies the set of roots of $\mathfrak a$ is periodic on $[x_0, 1]$
or coincides with it.

{\bf 2.} The periodicity of zeros of the function $\mathfrak a$ follows from its evenness and from the first assertion of the lemma.
Denote the distance between consecutive zeros by $\Delta$.

Then for $-1 \le x \le 1 - \Delta$ the following holds
$$\mathfrak a(x) = \mathfrak a(x) + \mathfrak a(1 - \Delta) \ge \mathfrak a(x + \Delta).$$

On the other hand, $-1 \le -(x + \Delta) \le 1 - \Delta$, and
$$\mathfrak a(x + \Delta) = \mathfrak a(-(x + \Delta)) + \mathfrak a(1 - \Delta) \ge \mathfrak a(-x) = \mathfrak a(x).$$

Thus, $\mathfrak a(x) = \mathfrak a(x + \Delta)$.
\end{proof}

\begin{lm}
\label{maxSumConcave}
Suppose that $\mathfrak a_1$ and $\mathfrak a_2$ satisfy $(\ref{almostConcave})$.
Then the function $\max (\mathfrak a_1(x), \mathfrak a_2(x))$ and $\mathfrak a_1(x) + \mathfrak a_2(x)$ also satisfy $(\ref{almostConcave})$.
\end{lm}
\begin{proof}
Set $\mathfrak a(x) = \max (\mathfrak a_1(x), \mathfrak a_2(x))$. Then
\begin{multline*}
\mathfrak a(1 + s + t) = \max(\mathfrak a_1( 1 + s + t), \mathfrak a_2(1 + s + t)) \le
\max(\mathfrak a_1(s) + \mathfrak a_1(t), \mathfrak a_2(s) + \mathfrak a_2(t)) \\
\le \max(\mathfrak a_1(s), \mathfrak a_2(s)) + \max(\mathfrak a_1(t), \mathfrak a_2(t)) =
\mathfrak a(s) + \mathfrak a(t).
\end{multline*}

The second part is obvious.
\end{proof}

\begin{lm}
\label{piecewiseLinearConcave}
Let the function $\mathfrak a$ satisfy $(\ref{almostConcave})$, $k \in \Nat$.
Then a piecewise linear function $\mathfrak a_k$,
interpolating $\mathfrak a$ using the nodes
$(-1 + \frac{2i}{k})$, $i = 0, 1, \dots, k$,
also satisfies $(\ref{almostConcave})$.
\end{lm}
\begin{proof}
{\bf 1.}
Let $s = -1 + \frac{2i}{k}$, $t = -1 + \frac{2j}{k}$.
Then the inequality $(\ref{almostConcave})$ holds for $\mathfrak a_k$, because it does for $\mathfrak a$,
and their values at these points are equal.

{\bf 2.}
Now let $s = -1 + \frac{2i}{k}$ and $t \in [-1 + \frac{2j}{k}, -1 + \frac{2(j + 1)}{k}]$.

Consider the linear function $h_1(t) = \mathfrak a_k( 1 - t + s ) - \mathfrak a_k(t) - \mathfrak a_k(s)$.
It follows from part 1 that $h_1(-1 + \frac{2j}{k}) \le 0$ and $h_1(-1 + \frac{2(j + 1)}{k}) \le 0$.
Since $h_1$ is linear, $h_1(t) \le 0$.
Thus, the inequality holds for every $s = -1 + \frac{2i}{k}$ and $t \in [-1, 1]$.

{\bf 3.}
Let $s$ and $t$ satisfy $1 - t + s = \frac{2j}{k}$.
Consider the function $h_2(y) = \mathfrak a_k(\frac{2j}{k}) - \mathfrak a_k(s + y) - \mathfrak a_k(t + y)$.
If we choose $y_0$ such that $s + y_0$ is one of the nodes then $t + y_0$ is also a node.
Therefore, $h_2(y_0) = \mathfrak a(\frac{2j}{k}) - \mathfrak a(s + y_0) - \mathfrak a(t + y_0) \le 0$.
Since $h_2$ is linear between such $y_0$'s, we obtain $h_2(y) \le 0$ for all admissible $y$.

{\bf 4.}
Finally, consider $h_3(s) = \mathfrak a_k( 1 - t + s ) - \mathfrak a_k(t) - \mathfrak a_k(s)$ for arbitrary given $t \in [-1, 1]$.
Note that parts 2 and 3 imply $h_3(s) \le 0$ for any $s$
such that either $s$ or $1 - t + s$ is a node.
Since $h_3$ is linear between these points, $h_3(s) \le 0$ for all admissible $s$,
and the statement follows.
\end{proof}


\section{The result for piecewise linear functions}

\rm
In this section we prove the inequality (\ref{toprove}) for piecewise linear functions.
Without loss of generality, we assume that $F(\cdot, 0) \equiv 0$.

\begin{thm}
\label{linth}
Let the function $\mathfrak a$ be even and satisfy the condition $(\ref{almostConcave})$.
Then, if $u$ is a nonnegative piecewise linear function then $I(\mathfrak a, u) \ge I(\mathfrak a, u^*)$.
\end{thm}

\begin{proof}
Let $-1 = x_1 < x_2 < \dots < x_K = 1$ be the nodes of $u$.
Consider the set $U$ equal to the range of $u$ with images of endpoints of linear pieces excluded:
$U := u( [-1, 1] ) \setminus \{ u(x_1), \dots, u(x_K) \}$.
It's obvious that the set $U$ is the union of a finite number of intervals $U = \cup_{j = 1}^N G_j$.

We denote by $m_j$ the number of preimages for $u_0 \in G_j$,
i.e. the number of solutions of the equation $u(y) = u_0$
(obviously, $m_j$ does not depend on $u_0 \in G_j$).
It is easy to see that the preimages are linear functions of $u_0$:
$y = y_k^j(u_0)$, $k = 1, \dots, m_j$,
and $y_k^j{}'(u(y)) = \frac{1}{u'(y)}$.
We assume that $y_1^j(u_0) < y_2^j(u_0) < \dots < y_{m_j}^j(u_0)$.

The solution of the equation $u^*(y^*)=u_0$ ($u_0 \in U$) can be expressed in terms of $y_k^j$:

\begin{center}
\begin{tabular}{l|l|l} 
\multirow{2}{*}{$u(-1)<u_0$ \rule[-34pt]{0pt}{65pt}} & $m_j$ is even & $y^*=1-\sum\limits_{k=1}^{m_j} (-1)^k y_k^j$ \rule[-17pt]{0pt}{40pt} \\
                                                     & $m_j$ is odd  & $y^*=-\sum\limits_{k=1}^{m_j} (-1)^k y_k^j$ \rule[-17pt]{0pt}{40pt} \\ \hline
\multirow{2}{*}{$u(-1)>u_0$ \rule[-34pt]{0pt}{65pt}} & $m_j$ is even & $y^*=-1+\sum\limits_{k=1}^{m_j} (-1)^k y_k^j$ \rule[-17pt]{0pt}{40pt} \\
                                                     & $m_j$ is odd  & $y^*=\sum\limits_{k=1}^{m_j} (-1)^k y_k^j$ \rule[-17pt]{0pt}{40pt} \\ 
\end{tabular}
\end{center}

Let $y^*(v) = (u^*)^{-1}(v)$.
Then $y^*{}'(v) = \sum_{k=1}^{m_j} \abs{y_k^j{}'(v)}$ for $v \in G_j$, as the signs in the expression for
$y^*$ and signs of $y_k^j{}'$ alternate, and $y^*{}'(v)\ge 0$.

The sets of zeros of $u'(x)$ and $u^*{}'(x)$ can have nonzero measure.
However, they do not contribute to the integral, since $F(u(x), 0) = 0$.

Consider the remaining parts of the integrals :
\begin{multline*}
I(\mathfrak a, u) = \sum_{j=1}^N \int_{u^{-1}(G_j)} F(u(x), \mathfrak a(x, u(x)) \abs{u'(x)}) dx
\\ = \sum_{j=1}^N \int_{G_j} \sum_{k=1}^{m_j} F\Big(v, \frac{\mathfrak a(y_k^j(v), v)}{\bigabs{y_k^j{}'(v)}}\Big) \bigabs{y_k^j{}'(v)} dv,
\end{multline*}
\begin{multline*}
I(\mathfrak a, u^*) = \sum_{j=1}^N \int_{(u^*)^{-1}(G_j)} F(u^*(x), \bigabs{\mathfrak a(x, u(x)) u^*{}'(x)}) dx
\\ = \sum_{j=1}^N \int_{G_j} F\Big(v, \frac{\mathfrak a(y^*(v), v)}{\sum_{k=1}^{m_j} \bigabs{y_k^j{}'(v)}}\Big)
\sum_{k=1}^{m_j} \bigabs{ y_k^j{}'(v) } dv.
\end{multline*}

We fix $j$ and $v$ in the right parts and prove the inequality for integrands.
We denote $b_k := |y_k^j{}'(v)|$, $y_k := y_k^j(v)$, $y^* := y^*(v)$, $m := m_j$.
Then the assertion takes the form:
$$T:=\sum_{k=1}^m b_k F\Big( v, \frac{ \mathfrak a(y_k, v) }{b_k} \Big)
\ge F\Big( v, \frac{ \mathfrak a(y^*, v) }{ \sum_{k=1}^m b_k  } \Big) \sum_{k=1}^m b_k.$$
By Jensen's inequality for the function $F(v, \cdot)$, we obtain
$$T \ge F\Big( v, \frac{ \sum_{k=1}^m \mathfrak a(y_k, v) }{ \sum_{k=1}^m b_k } \Big) \sum_{k=1}^m b_k.$$
Then it is sufficient to prove $\sum_{k=1}^m \mathfrak a(y_k, v) \ge \mathfrak a(y^*, v)$, which is true due to Lemma \ref{weightSum} and
Remark \ref{almostConcaveMultRem}.
\end{proof}

\begin{rem}
\label{landesLinear}
In the paper $\cite{Lan}$ the inequality $(\ref{toprove})$ is proved under the additional assumption $u(-1) = 0$
for the weight functions $\mathfrak a$, decreasing in $x$.
It is easy to see that under this assumption, the proof of Theorem $\ref{linth}$ works for weights satisfying
$(\ref{almostConcave})$ without the evenness assumption,
since in this case $u(-1) < u_0$, and we need only two of the four inequalities,
given by Lemma $\ref{weightSum}$.
It is also obvious that the assumption $(\ref{almostConcave})$ is weaker than the assumption of $\mathfrak a$ decreasing in $x$.
\end{rem}


\section{Extension of class of functions for which $I(a, u^*) \le I(a, u)$ holds}
\begin{lm}
Let the function $a$ be continuous. Then the functional $I(a, u)$ is weakly lower semicontinuous in $\W(-1, 1)$.
\label{lowersemi}
\end{lm}

\begin{proof}
Let $u_m \rightharpoondown u$ in $\W(-1, 1)$.
Let's denote $A = \varliminf I( a, u_m ) \ge 0$.
We are going to prove $I(a, u) \le A$.
In the case $A = \infty$ the assertion is trivial, so we can assume $A < \infty$.
Switching to a subsequence, we obtain $A = \lim I( a, u_m )$.
Weak convergence implies, that there exists
$R_0$ such that $\norm{ u_m }_{\W(-1, 1)} \le R_0$.

It is known that $\W(-1, 1)$ is compactly embedded in $L_1(-1, 1)$.
Then, switching to a subsequence, we can assume that $u_m \to u$ in $L_1(-1, 1)$
and $u_m(x) \to u(x)$ almost everywhere.
Then, by Egorov's theorem, for any $\eps$ there exists a set
$G_\eps^1$ such that $\abs{ G_\eps^1 } < \eps$ and $u_m \rightrightarrows u$ in $[-1, 1] \setminus G_\eps^1$.

Uniform convergence of $u_m$ implies $\exists K: \forall m>K \abs{u_m} \le \abs{u} + \eps$ in $[-1, 1] \setminus G_\eps^1$.
Let $G_\eps^2 = \{x \in [-1, 1] \setminus G_\eps^1 : \abs{u(x)} \ge \frac{R_0 + \eps}{\eps} \}$.
Then $$R_0 \ge \int_{-1}^1 \abs{ u(x) } dx \ge \int_{G_\eps^2} \abs{ u(x) } dx \ge
\int_{G_\eps^2} \frac{R_0 + \eps}{\eps} dx = \abs{G_\eps^2} \frac{R_0 + \eps}{\eps}$$
That is, $\abs{G_\eps^2} \le \eps \frac{R_0}{R_0 + \eps} < \eps$.
The functions $u_m$, $m > K$ converge uniformly and are uniformly bounded outside the set $G_\eps := G_\eps^1 \cup G_\eps^2$.

Continuity of $F$ and $a$ implies that for any $\eps$ and $R$, there exists
$N( \eps, R )$, such that if $x \in [-1, 1] \setminus G_\eps$, $\abs{ M } \le R$ and $m > N( \eps, R )$ then
$$| F( u_m( x ), a( x, u_m( x ) ) M ) - F( u( x ), a( x, u( x ) ) M ) | < \eps.$$

Let $E_{m,\eps} := \{ x \in [-1, 1]: \abs{ u_m'( x ) } \ge \frac{ R_0 }{ \eps } \}$.
Then
$$R_0 \ge \int_{-1}^1 \abs{ u_m'( x ) } dx \ge \int_{ E_{m,\eps} } \abs{ u_m'( x ) } dx \ge
\int_{ E_{m,\eps} } \frac{ R_0 }{ \eps } dx = \frac{ R_0 }{ \eps } \abs{ E_{m,\eps} }.$$
Therefore $\abs{ E_{m,\eps} } \le \eps$.

Finally we set $L_{m,\eps} := [-1, 1] \setminus ( E_{m,\eps} \cup G_\eps )$.
Note, that $\abs{ L_{m,\eps} } \ge 2 - 3 \eps$.

We fix $R := \frac{ R_0 }{ \eps }$, $N( \eps ) := N( \eps, \frac{ R_0 }{ \eps } )$.
For any $\eps > 0$, $x \in L_{m,\eps}$ and $m > N( \eps )$ we have
$$\Big | F( u_m( x ), a( x, u_m( x ) ) \abs{u_m'( x )} ) - F( u( x ), a( x, u( x ) ) \abs{u_m'( x )} ) \Big | < \eps,$$
thus
$$\int_{L_{m,\eps}} \Big | F( u_m( x ), a( x, u_m( x ) ) \abs{u_m'( x )} ) - F( u( x ), a( x, u( x ) ) \abs{u_m'( x )} ) \Big | dx < 2 \eps.$$

We put $\eps_j = \frac{ \eps }{ 2^j }$ ($j \ge 1$), $m_j = N( \eps_j ) + j \to \infty$ and $L_\eps = \bigcap L_{m_j,\eps_j}$.
Then $\sum \eps_j = \eps$ and $\abs{ [-1, 1] \setminus L_\eps } < 3 \eps$.
Therefore
$$\int_{L_\eps} \Big | F( u_{m_j}( x ), a( x, u_{m_j}( x ) ) |u_{m_j}'( x )| ) - F( u( x ), a( x, u( x ) ) |u_{m_j}'( x )| ) \Big | dx < 2 \eps_j.$$

And then
\begin{multline*}
A = \lim I (a, u_{m_j}) = \lim \int_{-1}^1 F(u_{m_j}(x), a(x, u_{m_j}(x)) | u_{m_j }'(x) |) dx \\
\ge \varliminf \int_{-1}^1 \chi_{L_\eps}(x) F(u (x), a(x, u(x)) | u_{m_j}'(x) |) dx
=: \varliminf J_\eps(u_{m_j}').
\end{multline*}

The new functional
$$J_\eps( v ) = \int_{-1}^1 \chi_{L_\eps}( x ) F( u( x ), a( x, u( x ) ) |v( x )| ) dx$$
is convex.
Switching to a subsequence $u_k$ again, we can assume that
$\varliminf J_\eps( u_{m_j}' ) = \lim J_\eps( u_k' )$.
Since $u_k' \rightharpoondown u'$ in $L_1$, we can choose a sequence
convex combinations of $u_k'$, which will converge to $u'$ strongly (see \cite[Theorem 3.13]{Rudin}).
Namely, there are $\alpha_{k,l} \ge 0$ for
$k \in \Nat$, $l \le k$, such that $\sum_{l = 1}^k \alpha_{k,l} = 1$ for every $k$ and
$w_k = \sum_{l = 1}^k \alpha_{k,l} u_{l}' \to u'$ in $L_1$.
Also, without loss of generality we can assume, that the minimal index $l$ of a nonzero coefficient $\alpha_{k,l}$
tends to infinity as $k$ tends to infinity.
Then
$$\lim J_\eps( u_k' ) = \lim \sum_{l = 1}^k \alpha_{k,l} J_\eps( u_{l}' ).$$

By the convexity of $J_\eps$, we have
$$\sum_{l = 1}^k \alpha_{k,l} J_\eps( u_{l}' ) \ge J_\eps( w_k ).$$

Finally, since $w_k \to u'$ in $L_1(-1, 1)$, we can assume, by switching to a subsequence, that $w_k(x) \to u'(x)$ almost everywhere.
Moreover, since $\abs{ u_j'( x ) } < \frac{ R_0 }{\eps}$ holds for $x \in L_\eps$, then $\abs{ w_k( x ) } < \frac{ R_0 }{\eps}$.
Hence,
$$F( u( x ), a( x, u( x ) ) w_k( x ) ) \le \max\limits_{(x, M)} F( u( x ), a( x, u( x ) ) M ) < \infty,$$
where the maximum is taken over a compact set
$(x,M) \in [-1, 1] \times [-\frac{ R_0 }{\eps},\frac{ R_0 }{\eps}]$.
Therefore, the Lebesgue theorem applies, and $\lim J_\eps(w_k) = J_\eps(u')$ is proved.
Thus,
$$A \ge \lim J_\eps( u_k' ) = \lim \sum_{l = 1}^k \alpha_{k,l} J_\eps( u_{l}' ) \ge
\varliminf J_\eps( w_k ) = J_\eps( u' ).$$

Since $\eps > 0$ is arbitrary, $A \ge I(a, u)$ is proved.
\end{proof}

\begin{lm}
\label{uplift}
Let $A \subset \W(-1,1)$.
And let $B \subset A$ contains only $v$ for which $I(a, v^*) \le I(a, v)$ holds.
Suppose that for each $u \in A$
there is a sequence $u_k \in B$ such that $u_k \to u$ in $\W(-1, 1)$ and
$I(a, u_k) \to I(a, u)$.
Then $\forall u \in A$ the inequality $I(a, u^*) \le I(a, u)$ holds.
\end{lm}
\begin{proof}
Let's pick some $u \in A$ and find the appropriate $u_k \in B$.
By hypothesis, $I(a, u_k^*) \le I(a, u_k) \to I(a, u)$.
It is shown in \cite[Theorem 1]{Br} that $u_k \to u$ in $\W(-1, 1)$ implies
$\overline{u_k} \rightharpoondown \overline{u}$ in $\W(-1, 1)$.
But $u_k^*( x ) = \overline{u_k}( \frac{x - 1}{2} )$ and
$u^*( x ) = \overline{u}( \frac{x - 1}{2} )$.
Hence, $u_k^* \rightharpoondown u^*$.
Then by the weak lower semicontinuity of the functional we conclude $I(a, u^*) \le \liminf I(a, u_k^*)$.
Thus $I(a, u^*) \le I(a, u)$.
\end{proof}

\begin{cor}
Let the weight $a$ be continuous, and the inequality $(\ref{toprove})$ hold for non-negative piecewise linear functions $u$.
Then it holds for all non-negative Lipschitz functions.
\end{cor}
\begin{proof}
By \cite{Gariepy}, Theorem 1 in Section 6.6, any Lipschitz function can be approximated with its derivative
almost everywhere with continuously differentiable functions.
Since the derivatives of the approximating functions are uniformly bounded,
the sequence will converge in $\W(-1, 1)$ due to Lebesgue theorem,
and also $I$ will converge.
In turn, continuously differentiable functions can be uniformly approximated with piecewise linear functions
along with their derivative.
This convergence provides convergence in $\W(-1, 1)$ and the convergence of the functional $I$.
Applying Lemma \ref{uplift} proves the corollary.
\end{proof}


\section{The transition to $\W$ functions with an additional restriction on weight}
\label{ASC}

In this section we obtain the inequality (\ref{toprove}) under the additional condition
monotony of the weight function at $x \in [-1, 0]$ and $x \in [0, 1]$.

\begin{lm}
\label{Wapprox}
Let $a$ --- continuous function, $a(\cdot, u)$ is increasing on $[-1, 0]$ and decreasing on $[0, 1]$ for all $u \ge 0$.
Then any function $u \in \W(-1, 1)$, $u \ge 0$,
Lipschitz functions approaching the functional $I$,
then there exists a sequence $u_k \in Lip[-1, 1]$, such that the relation $(\ref{convergence})$.
\end{lm}

\begin{proof}
We can assume that $I( a, u ) < \infty$.

Weight increases $a$ in $x$ at $x \in [-1, 0]$
and decreases for $x \in [0, 1]$.
We prove the assertion for the functional
$$I_2( u ) = \int_0^1 F( u(x), a(x, u(x)) |u'(x)| ) dx,$$
and the case of $[-1, 0]$ to first reduce the $$I_1( u ) = \int_{-1}^0 F( u( x ), a( x, u(x) ) |u'(x)| ) dx =
\int_0^1 F( u( -z ), a( -z, u(-z) ) |u'( -z )| ) dz.$$

To prove this, we will modify the schema from \cite [ Theorem 2.4]{ASC}.
Proof overlaps with \cite{ASC}, but for the reader's convenience, we present it here in full.

We need the following auxiliary assertion.

\begin{prop}
\label{convToOne}
$\cite[Lemma 2.7]{ASC}$.
Let $\phi_h: [-1, 1] \to \Real$ --- a sequence of Lipschitz functions satisfying the conditions :
$\phi_h' \ge 1$ for almost all $x$ and all $h$, $\phi_h( x ) \to x$ for almost every $x$.
Then, for any $f \in L_1(\Real)$ $f(\phi_h) \to f$ in $L_1(\Real)$.
\end{prop}

For $h \in \Nat$ cover the set $\{ x \in [0, 1]: |u'(x)| > h \}$
open set $A_h$.
Without loss of generality, we can assume that
$A_{h + 1} \subset A_{h}$ and $\abs{A_h} \to 0$ for $h \to \infty$.
As we take $v_h$ function coinciding with $u$ is the set $A_h$.
On connected sites will make $A_h$ $v_h$ linear.
Then $v_h \to u$ in $\W$.
Change little $v_h$, to make an approximation of Lipschitz.

Imagine $A_h = \cup_k \Omega_{h,k}$, where $\Omega_{h,k} = ( b_{h,k}^-, b_{h,k}^+ )$.
denote
$$\alpha_{h,k} := \abs{\Omega_{h,k}},\quad
\beta_{h,k} := v_h(b_{h,k}^+) - v_h(b_{h,k}^-) = u(b_{h,k}^+) - u(b_{h,k}^-).$$
Then $v'_h = \frac{\beta_{h,k}}{\alpha_{h,k}}$ in $\Omega_{h,k}$.
Note that
$$\sum_k \abs{\beta_{h,k}} \le \int_{A_h} \abs{u'} dx \le \norm{u'}_{L_1(-1, 1)}< \infty,$$
and hence
$\sum_k \abs{\beta_{h,k}} \to 0$ for $h \to 0$ by Lebesgue.

We define a function $\phi_h \in \W(0, 1)$ as follows:
$$
\begin{aligned}
\phi_h( 0 ) &= 0 & & \\
\phi_h' &=  1 & \text{ in } & [0, 1] \setminus A_h,\\
\phi_h' &=  \max \Big( \frac{ \abs{\beta_{h,k}} }{ \alpha_{h,k} }, 1 \Big) & \text{ in } & \Omega_{h,k}.
\end{aligned}
$$	

Note that $\int_0^1 \abs{\phi_h'} dx \le 1 + \sum_k \abs{\beta_{h,k}} < \infty$.

\medskip

We show that $\phi_h' \to 1$ in $L_1(0, 1)$ :
$$\int \abs{\phi_h' - 1} dx = \sum \Big( \max \Big( \frac{\abs{\beta_{h,k}}}{\alpha_{h,k}}, 1 \Big) - 1 \Big) \alpha_{h,k} \le
\sum \abs{\beta_{h,k}} \to 0.$$
It follows that $\phi_h$ satisfies the conditions of Proposition \ref{convToOne}.

We now consider the $\phi_h^{-1}: [0, 1] \to [0, 1]$ --- restriction inverse to $\phi_h$ functions on $[0, 1]$.
For it is true $0 \le ( \phi_h^{-1} )' \le 1$ and

$$
\begin{aligned}
\phi_h^{-1} ( 0 ) &= 0 & & \\
( \phi_h^{-1} )' &=  1 & \text{ in } & [0, 1] \setminus \phi_h( A_h ),\\
( \phi_h^{-1} )' &=  \min \Big( \frac{ \alpha_{h,k} }{ \abs{ \beta_{h,k} } }, 1 \Big) & \text{ in } & [0, 1] \cap \phi_h( \Omega_{h,k} ).
\end{aligned}
$$

Take $u_h = v_h( \phi_h^{-1} )$.
Note that $u_h(0) = u(0)$, and
\begin{align*}
u_h' &=  v_h'( \phi_h^{-1} ) \cdot ( \phi_h^{-1} )' = u'( \phi_h^{-1} ) & \text{ in } & [0, 1] \setminus \phi_h( A_h ),\\
u_h' &=  v_h'( \phi_h^{-1} ) \cdot ( \phi_h^{-1} )' = 
\sign{ \beta_{h,k} } \cdot \min \Big( 1, \frac{ \abs{ \beta_{h,k} } }{ \alpha_{h,k} } \Big) & \text{ in } & [0, 1] \cap \phi_h( \Omega_{h,k} ).
\end{align*}

Thus, $u_h$ Lipschitz as $u$ is a $A_h$ bounded derivative.

We show that $u_h \to u$ in $\W(0, 1)$.
To do this, it suffices to estimate

$$\norm{u_h' - u'}_{L_1} \le \int_{[0, 1] \setminus \phi_h(A_h)} \abs{u_h' - u'} + 
\int_{[0, 1] \cap \phi_h(A_h)} \abs{u_h'} + \int_{[0, 1] \cap \phi_h(A_h)} \abs{u'} =: P_h^1 + P_h^2 + P_h^3.$$
$$P_h^1 = \int_{[0, 1] \setminus \phi_h( A_h )} \abs{u'( \phi_h^{-1} ) - u'} dx =
\int_{\phi_h^{-1} ( [0, 1] ) \setminus A_h} \abs{u' - u'( \phi_h )} dz \le
\int_{[0, 1]} \abs{u' - u'( \phi_h )} dz.$$
By Proposition \ref{convToOne}, $P_h^1 \to 0$.

Further,
$$P_h^2 \le \abs{\phi_h( A_h )} = \sum \abs{\phi_h( \Omega_{h,k} )} = \sum \max (\abs{\beta_{h,k}}, \alpha_{h,k})
\le \sum \alpha_{h,k} + \sum \abs{\beta_{h,k}} \to 0.$$
Finally, $P_h^3 \to 0$ in absolute continuity of the integral, and the assertion is proved.

It remains to show that $I_2( u_h ) \to I_2( u )$.

$$I_2( u_h ) = \!\!\!\!\int\limits_{[0, 1] \setminus \phi_h( A_h )}\!\!\!\! F( u_h( x ), a( x, u_h(x) ) |u_h'( x )| ) dx +\
\!\!\!\!\int\limits_{[0, 1] \cap \phi_h( A_h )}\!\!\!\! F( u_h( x ), a( x, u_h(x) ) |u_h'( x )| ) dx.$$
These terms denote $\hat{P_h^1}$ and $\hat{P_h^2}$.
Since $u \in \W(0, 1)$, then $u \in L_\infty( [0, 1] )$. We denote $\norm{u}_\infty = r$,
then $\norm{u_h}_\infty < 2r$ for sufficiently large $h$. Additionally, $\abs{u_h'} \le 1$
almost everywhere in $\phi_h( A_h )$. Then $\hat{P_h^2} \le M_F \abs{\phi_h( A_h )} \to 0$, where
$$M_F = \max\limits_{[-2r, 2r] \times [-M_a, M_a]} F;\quad M_a = \max\limits_{[0, 1] \times [-2r, 2r]} a.$$

Further,
\begin{multline*}
\hat{P_h^1} = \int\limits_{ [0, 1] \setminus \phi_h( A_h ) }
	F( u( \phi_h^{-1}( x ) ), a( x, u( \phi_h^{-1}( x ) ) |u'( \phi_h^{-1}( x ) ) ( \phi_h^{-1} )'| ) dx
\\ =\int\limits_{ \phi_h^{-1}( [0, 1] ) \setminus A_h } F( u( z ), a( \phi_h( z ), u( z ) ) |u'( z )| ) dz
\\ = \int\limits_{ [0, 1] } F( u( z ), a( \phi_h( z ), u( z ) ) |u'( z )| ) \chi_{ \phi_h^{-1}( [0, 1] ) \setminus A_h }dz.
\end{multline*}
The last equality, generally speaking, does not make sense, since $\phi_h( z )$ can take values is $[0, 1]$.
We define $a( z, u ) = a( 1, u )$ at $z > 1$. Now the expression correctly.
Note that $\chi_{\phi_h^{-1}( [0, 1] ) \setminus A_h}$ increases, as set
$\phi_h^{-1}( [0, 1] )$ increases and $A_h$ decay, ie
$\phi_{h_1}^{-1}( [0, 1] ) \subset \phi_{h_2}^{-1}( [0, 1] )$ and $A_{h_1} \supset A_{h_2}$ for $h_1 \le h_2$.
On $[0, 1]$ (or even $\phi_h( [0, 1] )$) $a$ decreases, then $a( \phi_h( z ) )$ is expected to grow by $h$,
since $\phi_h( z )$ is decreasing in $h$. In this case it is possible to apply the theorem
monotone convergence and get
$$\hat{P_h^1} \to \int_{[0, 1]} F( u( z ), a( z, u( z ) ) |u'( z )| ) dz.$$

\end{proof}

\begin{rem}
Obviously, the same reasoning with binding function $u$ on the left end can be spent at any interval $[x_0, x_1]$, where
weight $a$ is decreasing in $x$. That is to get over this interval sequence
\begin{gather*}
u_h \to u \text{ in } \W(x_0, x_1);\\
\int_{x_0}^{x_1} F( u_h(x), a(x, u_h(x)) \abs{u_h'(x)} ) \to \int_{x_0}^{x_1} F( u(x), a(x, u(x)) \abs{u'(x)} ).
\end{gather*}
Similarly, if $a$ is increasing in $x$, can be approximated by $u$ with fixing on the right end.
\end{rem}

\begin{cor}
Let the function $a$ is continuous, even, satisfies $(\ref{almostConcave})$
and decreasing on $[0, 1]$. Then for every $u \in \W(-1, 1)$ holds $I( a, u^* ) \le I( a, u )$.
\end{cor}

\begin{proof}
Inequality follows immediately from Lemma \ref{uplift} and \ref{Wapprox}.
\end{proof}


\begin{thebibliography}{99}
\bibitem{ASC} G.~Alberti,~F.~Serra~Cassano: Non-occurrence of gap for one-dimentional autonomous functionals,
Proceedings of ``Calc. Var., Homogen. and Cont. Mech.'', G.~Bouchitt\'e, G.~Buttazzo, P.~Suquet, ed.: World Sci., Singapore, p.~1--17, 1994
\bibitem{Br} F.~Brock: Weighted Dirichlet-type inequalities for Steiner symmetrization,
Calc. Var. and PDEs~{\bf8}, p.~15--25, 1999
\bibitem{Kawohl} B.~Kawohl: Rearrangements and convexity of level sets in PDE,
Lecture notes in mathematics {\bf1150}. Berlin; Springer Verlag, 1985. 134~p.
\bibitem{Lan} R.~Landes: Some remarks on rearrangements and functionals with non-constant density,
Math.~Nachr.~{\bf280}, \No5--6, p.~560--570, 2007
\bibitem{DAN} С.~Банкевич, А.~Назаров: Об обобщении неравенства Пойа-Сеге для одномерных функционалов,
Доклады Академии Наук~{\bf438}, \No1, с.~11--13, 2011
\bibitem{BGH} Дж.~Буттаццо, М.~Джаквинта, С.~Гильдебрандт: Одномерные вариационные задачи. Введение,
Научная книга, Новосибирск, 2002. 246~с.
%G.~Buttazzo, M.~Giaquinta, S.~Hildebrandt: One-dimensional Variational Problems,
%Oxford Lecture Series in Mathematics and Its Applications {\bf15}, 1998. 272 p.
\bibitem{Zh1} В.~В.~Жиков: О весовых соболевских пространствах,
Матем. сб., {\bf189}, \No8, с.~27--58, 1998
\bibitem{Zh2} В.~В.~Жиков: К проблеме предельного перехода в дивергентных неравномерно эллиптических уравнениях,
Функц. анализ и его прил., {\bf35}, \No1, с.~23--39, 2001
\bibitem{LL} Э.~Либ, М.~Лосс: Анализ, Научная книга, Новосибирск, 1998. 276~с.
\bibitem{Rudin} У.~Рудин: Функциональный анализ, Мир, М., 1975. 444~с.
\bibitem{Fed} Г.~Федерер: Геометрическая теория меры, Наука, М., 1987. 760~с.
\bibitem{Gariepy} Л.~К.~Эванс, Р.~Ф.~Гариепи: Теория меры и тонкие свойства функций, Научная книга, Новосибирск, 2002. 216~с.
%\bibitem{Gariepy} L.~C.~Evans, R.~F.~Gariepy: 
%\bibitem{ET} И.~Экланд, Р.~Темам: Выпуклый анализ и вариационные проблемы, Мир, М., 1979. 400~с.

\end{thebibliography}

\end{document}
