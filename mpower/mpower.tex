\documentclass[12pt]{article}
\renewcommand{\baselinestretch}{1}
%\renewcommand{\baselinestretch}{1.5}
\textwidth=158mm
\textheight=232mm
\voffset=-24mm
%\pagestyle{empty}

\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{multirow}
\usepackage{color}

\newcommand{\Real}{\mathbb R}
\newcommand{\Nat}{\mathbb N}
\newcommand{\norm}[1]{\pmb{\Vert}#1\pmb{\Vert}}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\bignorm}[1]{\bigl\Vert#1\bigr\Vert}
\newcommand{\bigabs}[1]{\bigl\vert#1\bigr\vert}
\newcommand{\set}[1]{\left\{#1\right\}}
\renewcommand{\phi}{\varphi}
\newcommand{\eps}{\varepsilon}
\renewcommand{\ge}{\geqslant}
\renewcommand{\le}{\leqslant}
\renewcommand{\liminf}{\underline{\lim}}
\newcommand{\grad}{\triangledown}
\newcommand{\card}{{\rm card}}
\newtheorem{thm}{Theorem}
%\newtheorem{prop}{Предложение}
\newtheorem{lm}{Lemma}
%\newtheorem{rem}{Замечание}
%\newtheorem{cor}{Следствие}
\newcommand{\To}{\longrightarrow}
\newcommand{\Wf}{\stackrel{o\ }{W{}_1^1}}
\newcommand{\Wfp}{
\stackrel{o\ \ \ \ } {W{}_{\!p(x)}^1}
}
\newcommand{\W}{W_1^1}
\newcommand{\I}{\mathcal{I}}
\newcommand{\J}{\mathcal{J}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\B}{{\bf b}}
\newcommand{\sign}{\mathop{\rm sign}\nolimits}
\newcommand{\dist}{\mathop{\rm dist}\nolimits}
\newcommand{\comment}[1]{\colorbox{yellow}{#1}}
\newcounter{pictureCounter}

\title{On monotonicity of some functionals with variable exponent under symmetrization}
\author{
S.~Bankevich
\footnote{SPbU, Russia; Sergey.Bankevich@gmail.com}
%}
%\author{
\and
A.~I.~Nazarov
\footnote{PDMI RAS and SPbU, Russia; al.il.nazarov@gmail.com}
}
\date{}

\begin{document}
\maketitle

\section{Introduction}

Let $\Omega = \omega \times [-1, 1] \subset \Real^n$,
where $\omega$ is a bounded domain.
We denote $x = ( x_1, \dots, x_{n - 1}, y ) = ( x', y )$.

We consider two functionals on $\Wfp$
$$
\J(u) = \int\limits_\Omega | \nabla u(x) |^{p(x)} dx
\qquad
\I(u) = \int\limits_\Omega ( 1 + | \nabla u(x) |^2 )^{p(x) \over 2} dx.
$$

\begin{equation}
\label{toprove}
\I( u ) \ge \I( u^* )
\end{equation}

\section{Necessary conditions}

\begin{thm}
\label{uniform}
Suppose the inequality $\J(u^*) \le \J(u)$ holds for any Lipschitz function $u$.
Then $p(x', \cdot) \equiv const$.
\end{thm}
\begin{proof}
Take arbitrary $x_0 = (x_0', y_0) \in \omega \times (-1, 1)$.
For each $\alpha > 0$ and $\eps > 0$ such that $B( x_0, \eps ) \subset \Omega$,
we define
$$
u_{\alpha,\eps}(x) = \alpha ( \eps - |x - x_0| )_+.
$$
Then $u_{\alpha,\eps}^*(x) = \alpha ( \eps - |x - (x_0', 0)| )_+$ and

\begin{eqnarray*}
\begin{aligned}
& |u_{\alpha,\eps}'(x)|     = 0,      & x \in \Omega \setminus B( x_0, \eps ) \\
& |u_{\alpha,\eps}'(x)|     = \alpha, & x \in B( x_0, \eps ) \\
& |u_{\alpha,\eps}^*{}'(x)| = 0,      & x \in \Omega \setminus B( (x_0', 0), \eps ) \\
& |u_{\alpha,\eps}^*{}'(x)| = \alpha, & x \in B( (x_0', 0), \eps ) \\
\end{aligned}
\end{eqnarray*}

Thus, we have
$$
\J(u_{\alpha, \eps}) = \int\limits_{B( x_0, \eps )} \alpha^{p(x)} dx, \qquad
\J(u_{\alpha, \eps}^*) = \int\limits_{B( (x_0', 0), \eps )} \alpha^{p(x)} dx.
$$

From the theorem assumptions we have
$$
{\J(u_{\alpha, \eps}^*) \over |{B((x_0', 0), \eps)}|} \le {\J(u_{\alpha, \eps}) \over |{B(x_0, \eps)}|}.
$$
Passing to the limit $\eps \to 0$ in this inequality we have $\alpha^{p(x_0', 0)} \le \alpha^{p(x_0)}$ since $\alpha^{p(x)}$ is continuous.
Note, that $\alpha > 1$ and $\alpha < 1$ lead to $p(x_0', 0) \le p(x_0)$ and $p(x_0', 0) \ge p(x_0)$ conclusions respectively.
This concludes the proof of the theorem.

Note, that we can restrict functions $u$ to be piecewise linear.
\end{proof}

The theorem makes the direct generalization of inequality (\ref{PS}) impossible.

Let
$$
K_{x'}( y, s ) = s ( 1 + { 1 \over s^2 } )^{ p(x', y) \over 2 }.
$$
In \cite{power} we proved the following necessary condition for the one dimensional case (here we write $K$ instead of $K_{x'}$).
\begin{thm}
Suppose inequality \ref{toprove} holds for any piecewise linear $u$.
Then $p$ is even and convex in $y$, and $K$ is convex with respect to both variables.
\end{thm}

\begin{proof}
\end{proof}
Note, that convexity of $K$ implies convexity of $p$.

\section{Piecewise linear case}

We will need the following notations and lemma.
\begin{eqnarray*}
&& \B = ( b_1, \dots, b_{n - 1}, c ) = ( \B', c ) \\
&& \K_{x'}( y, \B', c ) = c \Big( 1 + { 1 + |\B'|^2 \over c^2 } \Big) ^{p(x', y) \over 2}.
\end{eqnarray*}
\begin{lm}
\label{quasiConv}
Suppose $K_{x'}$ is convex.
Then
$$
\sum\limits_{k = 1}^{m_j} \K_{x'} ( y_k, \B_k', c_k )
\ge 2 \K_{x'} \Big( { 1 \over 2 } \sum\limits_{k = 1}^{m_j} (-1)^k y_k, { 1 \over 2 } \sum\limits_{k = 1}^{m_j} (-1)^k \B_k', \sum\limits_{k = 1}^{m_j} c_k \Big).
$$
\end{lm}

\begin{proof}
Let $\overline{\K}_{x'} ( y, b, s ) = s ( 1 + { 1 + b^2 \over s^2 } ) ^{p(x', y) \over 2}$.
\end{proof}

\begin{thm}
Suppose $K_{x'}$ is convex with respect to both arguments.
Then for any piecewise linear $u$ the inequality $\I(u^*) \le \I(u)$ holds.
\end{thm}

\begin{proof}
Let $\partial \Omega \subset C \subset \Omega$ be a minimal closed set, such that on any connected subset of $\Omega \setminus C$ the function $u$ is linear.
Define
$$
U := u(\Omega) \setminus \{ ( x', u( x', y ) ): y \in (-1, 1), (x', y) \in C \}.
$$
Then the open set $U$ can be partitioned into a finite union of disjoint connected open sets $G_j$.
Let $m_j$ be the number of inverse images of $( x', u_0 ) \in G_j$, that is the number of solutions to $u( x', y ) = u_0$
(obviously, this number is same for any $( x', u_0 ) \in G_j$).
It is easy to see that the inverse images themselves are linear functions of $( x', u_0 )$:
$y = y_k^j( x', u_0 )$, $k = 1, \dots, m_j$,
and for $( x', y ) \in G_j$
$$
D_n u( x', y ) = \frac{1}{D_n y_k^j( x', u( x', y ) )},\qquad D_i u( x', y ) = -\frac{D_i y_k^j( x', u( x', y ) )}{D_n y_k^j( x', u( x', y ) )}.
%D_n y_k^j( x', u( x', y ) ) = \frac{1}{D_n u( x', y )},\qquad D_i y_k^j( x', u( x', y ) ) = -\frac{D_i u( x', y )}{D_n u( x', y )}.
$$
Without loss of generality, we assume that $y_1^j(x', u_0) < y_2^j(x', u_0) < \ldots < y_{m_j}^j(x', u_0)$.

The equation $u^*(x'_0, y^*) = u_0$ defines $y^*$ as a function of $( x'_0, u_0 ) \in G_j$ if we restrict $y^* \ge 0$.
The $y^*$ function can be expressed in terms of $y_k^j$ (in particular, $y^*$ is piecewise linear):
$$
y^*( x', u ) = {1 \over 2} \sum\limits_{k = 1}^{m_j} (-1)^k y_k^j.
$$
Hence it is clear that for $(x', y) \in G_j$
\begin{eqnarray*}
D_n y^*(x', u(x', y))
& = & \frac{1}{D_n u^*(x',y)}
= { 1 \over 2 } \sum\limits_{k=1}^{m_j}{|D_n y_k^j (x',u(x',y))|} \\
D_i y^*( x', y )
& = & { 1 \over 2 } \sum\limits_{k = 1}^{m_j} ( -1 )^k D_i y_k^j( x', u( x', y ) ).
\end{eqnarray*}

{\color{red} subtract zero derivative}

Then
\begin{multline*}
I(u)
= \int\limits_\Omega ( 1 + D_i u(x)^2 + D_n u(x)^2 )^{ p(x) \over 2 } dx' dy \\
= \sum\limits_j \int\limits_{G_j} \sum\limits_{k = 1}^{m_j} \Big(
    1 + { D_i y_k^j(x', u)^2 + 1 \over D_n y_k^j(x', u)^2 }
\Big) ^{ p( x', y_k^j(x,' u) ) \over 2 }
\times | D_n y_k^j(x', u)| dx' du \\
= \sum\limits_j \int\limits_{G_j} \sum\limits_{k = 1}^{m_j} \K_{x'} \big( y_k^j(x', u), D_i y_k^j(x', u), |D_n y_k^j(x', u)| \big) dx' du,
\end{multline*}

\begin{multline*}
I(u^*)
= 2 \int\limits_{ \{ ( x', y ) | ( x', y ) \in \Omega, y > 0 \} } ( 1 + D_i u^*(x)^2 + D_n u^*(x)^2 )^{ p(x) \over 2 } dx' dy \\
= 2 \sum\limits_j \int\limits_{G_j} \bigg(
    1 + { \big( {1 \over 2} \sum\limits_{k = 1}^{m_j} (-1)^k D_i y_k^j(x', u) \big)^2 + 1
    \over
    ( {1 \over 2} \sum\limits_{k = 1}^{m_j} |D_n y_k^j(x', u)| )^2 }
\bigg) ^{ p( x', y^*(x,' u) ) \over 2 }
\times {1 \over 2} \sum\limits_{k = 1}^{m_j} | D_n y_k^j(x', u) | dx' du \\
= 2 \sum\limits_j \int\limits_{G_j} \K_{x'} \Big(
    {1 \over 2} \sum\limits_{k = 1}^{m_j} (-1)^k y_k^j(x', u), {1 \over 2} \sum\limits_{k = 1}^{m_j} (-1)^k D_i y_k^j(x', u), {1 \over 2} \sum\limits_{k = 1}^{m_j} |D_n y_k^j(x', u)|
\Big) dx' du.
\end{multline*}

It is sufficient to settle the inequality for the integrants, thus, we need to settle
\begin{multline*}
\sum\limits_{k = 1}^{m_j} \K_{x'} \big( y_k^j(x', u), D_i y_k^j(x', u), |D_n y_k^j(x', u)| \big)
\\ \ge
\K_{x'} \Big(
    {1 \over 2} \sum\limits_{k = 1}^{m_j} (-1)^k y_k^j(x', u), {1 \over 2} \sum\limits_{k = 1}^{m_j} (-1)^k D_i y_k^j(x', u), {1 \over 2} \sum\limits_{k = 1}^{m_j} |D_n y_k^j(x', u)|
\Big).
\end{multline*}
But this inequality is secured by Lemma \ref{quasiConv}.
\end{proof}

Note, that necessary properties of $\K$ are inherited from similar properties of $K$:
\begin{lm}
Suppose $K( x, s )$ is convex.
Then $\K_{x'}( y, b_i, b_n )$ is also convex.
\end{lm}

\section{General case}


\end{document}