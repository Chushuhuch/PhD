\documentclass[12pt]{article}
\renewcommand{\baselinestretch}{1}
%\renewcommand{\baselinestretch}{1.5}
\textwidth=158mm
\textheight=232mm
\voffset=-24mm
%\pagestyle{empty}

\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{multirow}
\usepackage{color}

\newcommand{\Real}{\mathbb R}
\newcommand{\Nat}{\mathbb N}
\newcommand{\norm}[1]{\pmb{\Vert}#1\pmb{\Vert}}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\bignorm}[1]{\bigl\Vert#1\bigr\Vert}
\newcommand{\bigabs}[1]{\bigl\vert#1\bigr\vert}
\newcommand{\set}[1]{\left\{#1\right\}}
\renewcommand{\phi}{\varphi}
\newcommand{\eps}{\varepsilon}
\renewcommand{\ge}{\geqslant}
\renewcommand{\le}{\leqslant}
\renewcommand{\liminf}{\underline{\lim}}
\newcommand{\grad}{\triangledown}
\newcommand{\card}{{\rm card}}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lm}{Lemma}
%\newtheorem{rem}{Замечание}
%\newtheorem{cor}{Следствие}
\newcommand{\To}{\longrightarrow}
\newcommand{\Wf}{\stackrel{o\ }{W{}_1^1}}
\newcommand{\Wfp}{
\stackrel{o\ \ \ \ } {W{}_{\!p(x)}^1}
}
\newcommand{\W}{W_1^1}
\newcommand{\I}{\mathcal{I}}
\newcommand{\J}{\mathcal{J}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\B}{{\bf b}}
\newcommand{\sign}{\mathop{\rm sign}\nolimits}
\newcommand{\dist}{\mathop{\rm dist}\nolimits}
\newcommand{\comment}[1]{\colorbox{yellow}{#1}}
\newcounter{pictureCounter}

\title{On monotonicity of some functionals with variable exponent under symmetrization}
\author{
S.~Bankevich
\footnote{SPbU, Russia; Sergey.Bankevich@gmail.com}
%}
%\author{
\and
A.~I.~Nazarov
\footnote{PDMI RAS and SPbU, Russia; al.il.nazarov@gmail.com}
}
\date{}

\begin{document}
\maketitle

\section{Introduction}

Let $\Omega = \omega \times [-1, 1] \subset \Real^n$,
where $\omega$ is a bounded domain.
We denote $x = ( x_1, \dots, x_{n - 1}, y ) = ( x', y )$.

We consider two functionals on $\Wfp$
$$
\J(u) = \int\limits_\Omega | \nabla u(x) |^{p(x)} dx
\qquad
\I(u) = \int\limits_\Omega ( 1 + | \nabla u(x) |^2 )^{p(x) \over 2} dx.
$$

\section{Necessary conditions}

The following necessary conditions were announced in \cite{power} for one dimensional case.
However, the constructions for the multidimentional case are different,
thus we put full proofs here for the reader's convenience.

\begin{thm}
\label{uniform}
Suppose the inequality $\J(u^*) \le \J(u)$ holds for any Lipschitz function $u$.
Then $p(x', \cdot) \equiv const$.
\end{thm}
\begin{proof}
Take arbitrary $x_0 = (x_0', y_0) \in \omega \times (-1, 1)$.
For each $\alpha > 0$ and $\eps > 0$ such that $B( x_0, \eps ) \subset \Omega$,
we define
$$
u_{\alpha,\eps}(x) = \alpha ( \eps - |x - x_0| )_+.
$$
Then $u_{\alpha,\eps}^*(x) = \alpha ( \eps - |x - (x_0', 0)| )_+$ and

\begin{eqnarray*}
\begin{aligned}
& |u_{\alpha,\eps}'(x)|     = 0,      & x \in \Omega \setminus B( x_0, \eps ) \\
& |u_{\alpha,\eps}'(x)|     = \alpha, & x \in B( x_0, \eps ) \\
& |u_{\alpha,\eps}^*{}'(x)| = 0,      & x \in \Omega \setminus B( (x_0', 0), \eps ) \\
& |u_{\alpha,\eps}^*{}'(x)| = \alpha, & x \in B( (x_0', 0), \eps ) \\
\end{aligned}
\end{eqnarray*}

Thus, we have
$$
\J(u_{\alpha, \eps}) = \int\limits_{B( x_0, \eps )} \alpha^{p(x)} dx, \qquad
\J(u_{\alpha, \eps}^*) = \int\limits_{B( (x_0', 0), \eps )} \alpha^{p(x)} dx.
$$

From the theorem assumptions we have
$$
{\J(u_{\alpha, \eps}^*) \over |{B((x_0', 0), \eps)}|} \le {\J(u_{\alpha, \eps}) \over |{B(x_0, \eps)}|}.
$$
Passing to the limit $\eps \to 0$ in this inequality we have $\alpha^{p(x_0', 0)} \le \alpha^{p(x_0)}$ since $\alpha^{p(x)}$ is continuous.
Note, that $\alpha > 1$ and $\alpha < 1$ lead to $p(x_0', 0) \le p(x_0)$ and $p(x_0', 0) \ge p(x_0)$ conclusions respectively.
This concludes the proof of the theorem.

Note, that we can restrict functions $u$ to be piecewise linear.
\end{proof}

The theorem makes the direct generalization of inequality (\ref{PS}) impossible.

Let
$$
K((x', y), s) = K_{x'}( y, s ) = s ( 1 + { 1 \over s^2 } )^{ p(x', y) \over 2 }.
$$
\begin{thm}
Suppose inequality $\I(u^*) \le \I(u)$ holds for any piecewise linear $u$.
Then $p$ is even and convex in $y$, and $K_{x'}$ is convex with respect to both variables for each $x' \in \omega$.
\end{thm}

To prove the theorem we will need the following
\begin{prop}
\label{convProp}
{\rm \cite[Lemma 10]{1dim}.}
Suppose function $p$ is continuous on $[-1, 1]$, and
$$
\forall s, t \in [-1, 1] \qquad p(s) + p(t) \ge p\big({s - t \over 2}\big) + p\big({t - s \over 2}\big).
$$
Then $p$ is even and convex.
\end{prop}

\begin{proof}
Let $x_1 = (x'_0, y_1), x_2 = (x'_0, y_2)$ where $-1 < y_1 < y_2 < 1$ and $x'_0 \in \omega$.
We construct the function with support on a cylinder with the segment $[x_1, x_2]$ as an axis and radius $w$,
and with nonzero derivatives only in circumferences of the border of the cylinder.
Namely,
$$
u(x', y) = \min\Big( \big( {y - y_1 \over s} \big)_+, \big( {y_2 - y \over t} \big)_+, \delta ( w - |x' - x'_0| )_+, h \Big),
$$
where $s, t > 0$.
We suppose that $y_2 - y_1$ is much greater than $\delta$ and $h$ as well as the width of non-zero derivative area on the side of the cylinder $\varkappa = {h \over \delta}$.
We set $\varkappa = {w \over 2}$.
Also we suppose that derivative $\delta$ near the side of the cylinder is much less than derivatives ${1 \over s}$ and ${1 \over t}$ near its bases.
In particular, $h = \delta \varkappa < \min( {1 \over s}, {1 \over t } ) {y_2 - y_1 \over 2}$.
It is easy to see that $u^*$ has similar shape with $y_1, y_2$ substituted by ${y_1 - y_2 \over 2}, {y_2 - y_1 \over 2}$, and both $s$ and $t$ changed to ${s + t \over 2}$.

To evaluate $I(u)$ and $I(u^*)$ we calculate areas $A_s$ with gradient ${1 \over s}$ (for ${1 \over t}$ and ${2 \over s + t}$ it is calculated the same way)
and $A_\delta$ with gradient $\delta$:
\begin{multline*}
\begin{split}
& A_s & = & \delta s \int\limits_{ w \over 2}^w |B^{n - 1}_r| dr = C \delta s \big( w^n - \big({ w \over 2}\big)^n \big) \\
& A_\delta & = & \int\limits_{ w \over 2}^w |S^{n - 1}_r| ( y_2 - y_1 - (w - r) \delta (s + t) ) dr
\end{split}
\\
= C (y_2 - y_1) \big( w^{n - 1} - \big({ w \over 2}\big)^{n - 1} \big)
- C \delta (s + t) \big( { w^n - w ({ w \over 2})^{n - 1} \over n - 1 } - { w^n - ({ w \over 2})^n \over n } \big)
\\
= C (y_2 - y_1) \big( w^{n - 1} - \big({ w \over 2}\big)^{n - 1} \big)
- C \delta (s + t) \big( w^n - \big({ w \over 2}\big)^{n - 1} ( w + { w \over 2} (n - 1) ) \big)
\end{multline*}
for some absolute constants $C$.
Note, that $A_\delta$ is the area of gradient $\delta$ for $u^*$ as well.

Take ${1 \over s} = {1 \over t} = w^2$, $\delta = w^4$ and set $w \to 0$.
Then
\begin{eqnarray*}
& A_s & = C w^{n + 2} \\
& A_\delta & = C w^{n - 1} ( 1 + o(1) )
\end{eqnarray*}
and
\begin{multline*}
0 \le ( I(u) - (y_2 - y_1) |B^{n - 1}| ) - ( I(u^*) - (y_2 - y_1) |B^{n - 1}| )
\\ \le ( ( 1 + w^4 )^{p(\bar{x}_1) \over 2} - 1 ) A_s + ( ( 1 + w^4 )^{p(\bar{x}_2) \over 2} - 1 ) A_t
+ ( ( 1 + w^8 )^{P \over 2} - 1 ) A_\delta
\\- ( ( 1 + w^4 )^{p(\hat{x}_1) \over 2} - 1 + ( 1 + w^4 )^{p(\hat{x}_2) \over 2} - 1 ) A_{s + t \over 2}
- ( ( 1 + w^8 )^{1 \over 2} - 1 ) A_\delta
\\ \le \Big( w^4 \big( {p(\bar{x}_1) \over 2} + o(1) \big) + w^4 \big( {p(\bar{x}_2) \over 2} + o(1) \big) \Big) C w^{n + 2}
+ w^8 \big( {P \over 2} + o(1) \big) C w^{n - 1} ( 1 + o(1) )
\\ -\Big( w^4 \big( {p(\hat{x}_1) \over 2} + o(1) \big) + w^4 \big( {p(\hat{x}_2) \over 2} + o(1) \big) \Big) C w^{n + 2}
- w^8 \big( {1 \over 2} + o(1) \big) C w^{n - 1} ( 1 + o(1) )
\\ = C w^{n + 6} ( p(\bar{x}_1) + p(\bar{x}_2) - p(\hat{x}_1) - p(\hat{x}_2) + o(1) ).
\end{multline*}
Here $P = \max p(x', y)$,
and $\bar{x}_1$, $\bar{x}_2$, $\hat{x}_1$, $\hat{x}_2$ are some points in
$w$-circumference of $x_1$, $x_2$, $(x'_0, {y_1 - y_2 \over 2})$, $(x'_0, {y_2 - y_1 \over 2})$ respectively.
Passing to the limit $w \to 0$ and applying Lemma \ref{convProp}
we conclude that $p(x', \cdot)$ is even and convex.

Now take arbitrary $s$ and $t$, $\delta = w^2$ and set $w \to 0$.
Then
\begin{eqnarray*}
& A_s & = C s w^{n + 2} \\
& A_\delta & = C w^{n - 1} ( 1 + o(1) )
\end{eqnarray*}
and
\begin{multline*}
0 \le ( I(u) - (y_2 - y_1) |B^{n - 1}| ) - ( I(u^*) - (y_2 - y_1) |B^{n - 1}| )
\\ \le ( ( 1 + s^2 )^{p(\bar{x}_1) \over 2} - 1 ) A_s + ( ( 1 + t^2 )^{p(\bar{x}_2) \over 2} - 1 ) A_t
+ ( ( 1 + w^4 )^{P \over 2} - 1 ) A_\delta
\\- \Big( \Big( 1 + \big({s + t \over 2}\big)^2 \Big)^{p(\hat{x}_1) \over 2} - 1 + \Big( 1 + \big({s + t \over 2}\big)^2 \Big)^{p(\hat{x}_2) \over 2} - 1 \Big) A_{s + t \over 2}
- ( ( 1 + w^4 )^{1 \over 2} - 1 ) A_\delta
\\ = ( K(\bar{x}_1, s) + K(\bar{x}_2, t) - K(\hat{x}_1, {s + t \over 2}) - K(\hat{x}_2, {s + t \over 2}) ) C w^{n + 2}
+ ( 2 A_{s + t \over 2} - A_s - A_t )
\\ + C w^{n + 3} (1 + o(1))
= ( K(\bar{x}_1, s) + K(\bar{x}_2, t) - K(\hat{x}_1, {s + t \over 2}) - K(\hat{x}_2, {s + t \over 2}) ) C w^{n + 2} ( 1 + o(1) ).
\end{multline*}
Here $P$, $\bar{x}_1$, $\bar{x}_2$, $\hat{x}_1$, $\hat{x}_2$ are as earlier.
Passing to the limit $w \to 0$ and applying evenness of $p$ in $y$ we obtain the convexity of $K_{x_0'}$.

\end{proof}
Note, that convexity of $K$ implies convexity of $p$.

\section{Piecewise linear case}

We will need the following notations and lemma.
\begin{eqnarray*}
&& \B = ( b_1, \dots, b_{n - 1}, c ) = ( \B', c ) \\
&& \K_{x'}( y, \B', c ) = c \Big( 1 + { 1 + |\B'|^2 \over c^2 } \Big) ^{p(x', y) \over 2}.
\end{eqnarray*}
\begin{lm}
\label{quasiConv}
Suppose $K_{x'}$ is convex.
Then
$$
\sum\limits_{k = 1}^{m_j} \K_{x'} ( y_k, \B_k', c_k )
\ge 2 \K_{x'} \Big( { 1 \over 2 } \sum\limits_{k = 1}^{m_j} (-1)^k y_k, { 1 \over 2 } \sum\limits_{k = 1}^{m_j} (-1)^k \B_k', { 1 \over 2 } \sum\limits_{k = 1}^{m_j} c_k \Big).
$$
\end{lm}

\begin{proof}
Let $\overline{\K}_{x'} ( y, b, s ) = s ( 1 + { 1 + b^2 \over s^2 } ) ^{p(x', y) \over 2}$.
\end{proof}

\begin{thm}
Suppose $K_{x'}$ is convex with respect to both arguments.
Then for any piecewise linear $u$ the inequality $\I(u^*) \le \I(u)$ holds.
\end{thm}

\begin{proof}
Let $\partial \Omega \subset C \subset \Omega$ be a minimal closed set, such that on any connected subset of $\Omega \setminus C$ the function $u$ is linear.
Define
$$
U := u(\Omega) \setminus \{ ( x', u( x', y ) ): y \in (-1, 1), (x', y) \in C \}.
$$
Then the open set $U$ can be partitioned into a finite union of disjoint connected open sets $G_j$.
Let $m_j$ be the number of inverse images of $( x', u_0 ) \in G_j$, that is the number of solutions to $u( x', y ) = u_0$
(obviously, this number is same for any $( x', u_0 ) \in G_j$).
It is easy to see that the inverse images themselves are linear functions of $( x', u_0 )$:
$y = y_k^j( x', u_0 )$, $k = 1, \dots, m_j$,
and for $( x', y ) \in G_j$
$$
D_n u( x', y ) = \frac{1}{D_n y_k^j( x', u( x', y ) )},\qquad D_i u( x', y ) = -\frac{D_i y_k^j( x', u( x', y ) )}{D_n y_k^j( x', u( x', y ) )}.
%D_n y_k^j( x', u( x', y ) ) = \frac{1}{D_n u( x', y )},\qquad D_i y_k^j( x', u( x', y ) ) = -\frac{D_i u( x', y )}{D_n u( x', y )}.
$$
Without loss of generality, we assume that $y_1^j(x', u_0) < y_2^j(x', u_0) < \ldots < y_{m_j}^j(x', u_0)$.

The equation $u^*(x'_0, y^*) = u_0$ defines $y^*$ as a function of $( x'_0, u_0 ) \in G_j$ if we restrict $y^* \ge 0$.
The $y^*$ function can be expressed in terms of $y_k^j$ (in particular, $y^*$ is piecewise linear):
$$
y^*( x', u ) = {1 \over 2} \sum\limits_{k = 1}^{m_j} (-1)^k y_k^j.
$$
Hence it is clear that for $(x', y) \in G_j$
\begin{eqnarray*}
D_n y^*(x', u(x', y))
& = & \frac{1}{D_n u^*(x',y)}
= { 1 \over 2 } \sum\limits_{k=1}^{m_j}{|D_n y_k^j (x',u(x',y))|} \\
D_i y^*( x', y )
& = & { 1 \over 2 } \sum\limits_{k = 1}^{m_j} ( -1 )^k D_i y_k^j( x', u( x', y ) ).
\end{eqnarray*}

{\color{red} subtract zero derivative}

Then
\begin{multline*}
I(u)
= \int\limits_\Omega ( 1 + D_i u(x)^2 + D_n u(x)^2 )^{ p(x) \over 2 } dx' dy \\
= \sum\limits_j \int\limits_{G_j} \sum\limits_{k = 1}^{m_j} \Big(
    1 + { D_i y_k^j(x', u)^2 + 1 \over D_n y_k^j(x', u)^2 }
\Big) ^{ p( x', y_k^j(x,' u) ) \over 2 }
\times | D_n y_k^j(x', u)| dx' du \\
= \sum\limits_j \int\limits_{G_j} \sum\limits_{k = 1}^{m_j} \K_{x'} \big( y_k^j(x', u), D_i y_k^j(x', u), |D_n y_k^j(x', u)| \big) dx' du,
\end{multline*}

\begin{multline*}
I(u^*)
= 2 \int\limits_{ \{ ( x', y ) | ( x', y ) \in \Omega, y > 0 \} } ( 1 + D_i u^*(x)^2 + D_n u^*(x)^2 )^{ p(x) \over 2 } dx' dy \\
= 2 \sum\limits_j \int\limits_{G_j} \bigg(
    1 + { \big( {1 \over 2} \sum\limits_{k = 1}^{m_j} (-1)^k D_i y_k^j(x', u) \big)^2 + 1
    \over
    ( {1 \over 2} \sum\limits_{k = 1}^{m_j} |D_n y_k^j(x', u)| )^2 }
\bigg) ^{ p( x', y^*(x,' u) ) \over 2 }
\times {1 \over 2} \sum\limits_{k = 1}^{m_j} | D_n y_k^j(x', u) | dx' du \\
= 2 \sum\limits_j \int\limits_{G_j} \K_{x'} \Big(
    {1 \over 2} \sum\limits_{k = 1}^{m_j} (-1)^k y_k^j(x', u), {1 \over 2} \sum\limits_{k = 1}^{m_j} (-1)^k D_i y_k^j(x', u), {1 \over 2} \sum\limits_{k = 1}^{m_j} |D_n y_k^j(x', u)|
\Big) dx' du.
\end{multline*}

It is sufficient to settle the inequality for the integrants, thus, we need to prove
\begin{multline*}
\sum\limits_{k = 1}^{m_j} \K_{x'} \big( y_k^j(x', u), D_i y_k^j(x', u), |D_n y_k^j(x', u)| \big)
\\ \ge
\K_{x'} \Big(
    {1 \over 2} \sum\limits_{k = 1}^{m_j} (-1)^k y_k^j(x', u), {1 \over 2} \sum\limits_{k = 1}^{m_j} (-1)^k D_i y_k^j(x', u), {1 \over 2} \sum\limits_{k = 1}^{m_j} |D_n y_k^j(x', u)|
\Big).
\end{multline*}
But this inequality is secured by Lemma \ref{quasiConv}.
\end{proof}

Note, that necessary properties of $\K$ are inherited from similar properties of $K$:
\begin{lm}
Suppose $K( x, s )$ is convex.
Then $\K_{x'}( y, b_i, b_n )$ is also convex.
\end{lm}

\section{General case}


\end{document}