\documentclass[12pt]{article}
\renewcommand{\baselinestretch}{1}
%\renewcommand{\baselinestretch}{1.5}
\textwidth=158mm
\textheight=232mm
\voffset=-24mm
%\pagestyle{empty}

\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{color}

\newcommand{\Real}{\mathbb R}
\newcommand{\Nat}{\mathbb N}
\newcommand{\norm}[1]{\pmb{\Vert}#1\pmb{\Vert}}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\bignorm}[1]{\bigl\Vert#1\bigr\Vert}
\newcommand{\bigabs}[1]{\bigl\vert#1\bigr\vert}
\newcommand{\set}[1]{\left\{#1\right\}}
\renewcommand{\phi}{\varphi}
\newcommand{\eps}{\varepsilon}
\renewcommand{\ge}{\geqslant}
\renewcommand{\le}{\leqslant}
\renewcommand{\liminf}{\underline{\lim}}
\newcommand{\grad}{\triangledown}
\newcommand{\card}{{\rm card}}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lm}{Lemma}
%\newtheorem{rem}{Замечание}
%\newtheorem{cor}{Следствие}
\newcommand{\To}{\longrightarrow}
\newcommand{\Wf}{\stackrel{o\ }{W{}_1^1}}
\newcommand{\Wfp}{
\stackrel{o\ \ \ \ } {W{}_{\!p(x)}^1}
}
\newcommand{\W}{W_1^1}
\newcommand{\I}{\mathcal{I}}
\newcommand{\J}{\mathcal{J}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\B}{{\bf b}}
\newcommand{\sign}{\mathop{\rm sign}\nolimits}
\newcommand{\dist}{\mathop{\rm dist}\nolimits}
\newcommand{\supp}{\mathop{\rm supp}}
\newcommand{\comment}[1]{\colorbox{yellow}{#1}}
\newcounter{pictureCounter}

\title{On monotonicity of some functionals with variable exponent under symmetrization}
\author{
S.~Bankevich
\footnote{SPbU, Russia; Sergey.Bankevich@gmail.com}
%}
%\author{
\and
A.~I.~Nazarov
\footnote{PDMI RAS and SPbU, Russia; al.il.nazarov@gmail.com}
}
\date{}

\begin{document}
\maketitle

\section{Introduction}

\begin{equation}
\label{PS}
I(u^*) \le I(u), \qquad \text{where } I(u) = \int\limits_{-1}^1 |u'(x)|^p dx, \quad p \ge 1
\end{equation}

\begin{equation*}
\J(u) = \int\limits_{-1}^1 |u'(x)|^{p(x)} dx, \qquad \I(u) = \int\limits_{-1}^1 ( 1 + | u'(x) |^2 )^{\frac {p(x)}{2}} dx,
\end{equation*}
Where $p(x) \ge 1$ is a continuous function defined on $[-1, 1]$, $u \in \Wf[-1, 1]$.


\section{Necessary conditions}

\begin{thm}
\label{uniform}
Suppose $\J(u^*) \le \J(u)$ holds for any piecewise linear $u$.
Then $p(x) \equiv const$.
\end{thm}

\begin{proof}
Consider a point $x_0 \in (-1, 1)$. For every $\alpha > 0$ and $\eps > 0$ such that $[x_0 - \eps, x_0 + \eps] \subset [-1, 1]$ we define
$$
u_{\alpha,\eps}(x) = \alpha ( \eps - |x - x_0| )_+.
$$
Then $u_{\alpha,\eps}^*(x) = \alpha ( \eps - |x| )_+$, and
$$
\J(u_{\alpha, \eps}) = \int\limits_{x_0 - \eps}^{x_0 + \eps} \alpha^{p(x)} dx, \qquad
\J(u_{\alpha, \eps}^*) = \int\limits_{-\eps}^{\eps} \alpha^{p(x)} dx.
$$
We take the inequality
$$
\frac {\J(u_{\alpha, \eps}^*)}{2 \eps} \le \frac {\J(u_{\alpha, \eps})} {2 \eps},
$$
and push $\eps$ to zero. Since $p$ is continuous, this gives $\alpha^{p(0)} \le \alpha^{p(x_0)}$.
Taking $\alpha > 1$ and $\alpha < 1$ we arrive at $p(0) \le p(x_0)$ and $p(0) \ge p(x_0)$ respectively.
\end{proof}

%Thus, a non-trivial extension of (\ref{PS}) does not exist.

\begin{thm}
\label{necessary}
Suppose inequality $\I(u^*) \le \I(u)$ holds for any piecewise linear $u$.
Then $p$ is even and convex.
Moreover, the following function is convex:
$$K(s, x) = s ( 1 + s^{-2} )^{\frac {p(x)}{2}} \qquad s > 0,\ x \in [-1, 1].$$
\end{thm}

To prove Theorem \ref{necessary} we need the following
\begin{prop}
\label{convProp}
{\rm \cite[Lemma 10]{1dim}.}
Suppose $p$ is continuous function on $[-1, 1]$ and suppose
$$
\forall s, t \in [-1, 1] \qquad p(s) + p(t) \ge p\big(\frac {s - t}{2}\big) + p\big(\frac{t - s}{2}\big).
$$
Then $p$ is even and convex.
\end{prop}

\begin{proof}[Proof of Theorem \ref{necessary}]
Fix two points $-1 < x_1 < x_2 < 1$
and consider a finite piecewise linear function with nonzero derivative only in $x_1$ and $x_2$ circumferences.
Namely, for arbitrary $s, t > 0$ and a small enough $\eps > 0$,
$$
u_\eps(x) = \min\big( 2 \eps, ( \eps + s^{-1} (x - x_1))_+, ( \eps + t^{-1} (x_2 - x) )_+\big).
$$
Then
$$
u_\eps^*(x) = \min\big( 2 \eps, ( \eps + ( t + s )^{-1} ( x_2 - x_1 - 2 |x| ) )_+ \big).
$$

%\begin{center}
%\begin{picture}(200,90)
%\refstepcounter{pictureCounter}
%\label{uGraph}
%
%\put(0,25){\vector(1,0){200}}
%\put(100,15){\vector(0,1){80}}
%\put(10,24){\line(0,1){2}}
%\put(0,14){$-1$}
%\put(190,24){\line(0,1){2}}
%\put(188,14){$1$}
%
%\put(38,25){\line(1,1){48}}
%\put(38,24){\line(0,1){2}}
%\put(20,14){$x_1 - s$}
%\put(86,24){\line(0,1){2}}
%\put(68,30){$x_1 + s$}
%
%\put(86,73){\line(1,0){48}}
%
%\put(134,73){\line(2,-3){32}}
%\put(134,24){\line(0,1){2}}
%\put(116,30){$x_2 - t$}
%\put(166,24){\line(0,1){2}}
%\put(148,14){$x_2 + t$}
%
%\put(110,77){$2 \eps$}
%\put(45,70){$u(x)$}
%
%\put(85,1){Fig. \arabic{pictureCounter}}
%\end{picture}
%\end{center}

%Тогда функция $u^*$ имеет следующий вид:
%\begin{equation}
%\left\{
%\begin{aligned}
%u^*(x) &= 0, & x &\in [-1, { x_2 - x_1 \over 2 } - { s + t \over 2 } \eps] \cup [{ x_1 - x_2 \over 2 } + { s + t \over 2 } \eps, 1]\\
%u^*(x) &= 2 \eps, & x &\in [{ x_2 - x_1 \over 2 } + { s + t \over 2 } \eps, { x_1 - x_2 \over 2 } - { s + t \over 2 } \eps]\\
%u^*(x) &= \eps + \frac{ x - { x_2 - x_1 \over 2 } }{ { s + t \over 2 } }, & x &\in [{ x_2 - x_1 \over 2 } - { s + t \over 2 } \eps, { x_2 - x_1 \over 2 } + { s + t \over 2 } \eps]\\
%u^*(x) &= \eps - \frac{ x - { x_1 - x_2 \over 2 } }{ { s + t \over 2 } }, & x &\in [{ x_1 - x_2 \over 2 } - { s + t \over 2 } \eps, { x_1 - x_2 \over 2 } + { s + t \over 2 } \eps]\\
%\end{aligned}
%\right.
%\end{equation}
%
%\begin{center}
%\begin{picture}(200,90)
%\refstepcounter{pictureCounter}
%\label{uGraph}
%
%\put(0,25){\vector(1,0){200}}
%\put(100,15){\vector(0,1){80}}
%\put(10,24){\line(0,1){2}}
%\put(0,14){$-1$}
%\put(190,24){\line(0,1){2}}
%\put(188,14){$1$}
%
%\put(36,25){\line(5,6){40}}
%
%\put(76,73){\line(1,0){48}}
%
%\put(124,73){\line(5,-6){40}}
%
%\put(110,77){$2 \eps$}
%\put(35,70){$u^*(x)$}
%
%\put(85,1){Fig. \arabic{pictureCounter}}
%\end{picture}
%\end{center}

%Полагая $s = t$, из неравенства $\I(u^*) \le \I(u)$ получим $p(x_1) + p(x_2) \le p({ x_1 - x_2 \over 2 }) + p({ x_2 - x_1 \over 2 })$.
%Можно проверить (\cite[Lemma 10]{1dim}), что отсюда следует четность и выпуклость функции $p$.
%Далее, рассматривая произвольные $s$ и $t$, получаем выпуклость функции $K$.

Then $\I(u^*) \le \I(u)$ leads to
\begin{multline*}
\int\limits_{ x_1 - s \eps }^{ x_1 + s \eps } \big( 1 + \frac 1 {s^2 } \big)^{\frac {p(x)} 2} dx
+ \int\limits_{ x_2 - t \eps }^{ x_2 + t \eps } \big( 1 + \frac 1 {t^2 } \big)^{\frac {p(x)} 2} dx \ge
\\ \ge \int\limits_{ \frac { x_2 - x_1} 2  - \frac {s + t}2 \eps }^{ { \frac {x_2 - x_1}2 } + { \frac {s + t} 2 } \eps }
\Big( 1 + \frac{1}{ ( {\frac { s + t} 2 } )^2 } \Big)^{\frac {p(x)} 2} dx
     + \int\limits_{ {\frac { x_1 - x_2} 2 } - { \frac {s + t} 2 } \eps }^{ { \frac {x_1 - x_2} 2 } + { \frac {s + t} 2 } \eps }
     \Big( 1 + \frac{1}{ ( { \frac {s + t} 2 } )^2 } \Big)^{\frac {p(x)} 2} dx
\end{multline*}

Passage to the limit as $\eps \to 0$ gives
\begin{equation}
\label{preConv}
s \Big( 1 + { \frac 1 {s^2} } \Big)^{\frac {p(x_1)} 2} + t \Big( 1 + { \frac 1 {t^2} } \Big)^{\frac {p(x_2)} 2}
\ge { \frac {s + t} 2 } \Big( 1 + \frac{1}{ ( { \frac {s + t} 2 } )^2 } \Big)^{\frac {p( { \frac {x_2 - x_1} 2 } )} 2}
  + { \frac {s + t} 2 } \Big( 1 + \frac{1}{ ( { \frac {s + t} 2 } )^2 } \Big)^{\frac {p( { \frac {x_1 - x_2} 2 } )} 2}.
\end{equation}

First, we put $s = t$ in (\ref{preConv}). This gives
\begin{equation}
\label{s_eq_t}
( 1 + { \frac 1 {s^2} } )^{\frac {p(x_1)} 2} + ( 1 + { \frac 1 {s^2} } )^{\frac {p(x_2)} 2}
\ge ( 1 + { \frac 1 {s^2} } )^{\frac {p( { \frac {x_2 - x_1} 2 } )}  2} + ( 1 + { \frac 1 {s^2} } )^{\frac {p( { \frac {x_1 - x_2}2 } )} 2}.
\end{equation}

We define $\sigma: = {\frac 1 {s^2}}$, apply Taylor's expansion at $\sigma = 0$ in (\ref{s_eq_t}) and arrive at
$$
\sigma p(x_1) + \sigma p(x_2) \ge \sigma p( {\frac { x_2 - x_1}2 } ) + \sigma p( {\frac { x_1 - x_2} 2 } ) + r(\sigma),
$$
where $r(\sigma) = o(\sigma)$ as $\sigma \to 0$.
Hence for any $x_1, x_2 \in [-1, 1]$ we have
$$
p(x_1) + p(x_2) \ge p( { \frac {x_2 - x_1} 2 } ) + p( { \frac {x_1 - x_2} 2 } ).
$$
By Proposition \ref{convProp}, $p$ is even and convex.

Finally we put $-x_2$ instead of $x_2$ in (\ref{preConv}). Since $p$ is even, we obtain
$K(s, x_1) + K(t, x_2) \ge 2 K( { \frac {s + t}2 }, {\frac { x_1 + x_2}2 } )$.
\end{proof}


\section{Proof of inequality $\I(u^*) \le \I(u)$}

In this section we show that necessary conditions from Theorem \ref{necessary} are sufficient as well.

\begin{lm}
\label{quasiConv}
Let $m$ be an even positive number, let $s_k > 0$ ($k = 1 \dots m$), and let $-1 \le x_1 \le \dots \le x_m \le 1$.
Suppose that $K(s, x)$ is even in $x$ and jointly convex in both arguments.
Then
\begin{equation}
\label{K_ineq}
\sum\limits_{k = 1}^{m} K(s_k, x_k) \ge
2 K\Big({\frac 1 2} \sum\limits_{k = 1}^{m} s_k, {\frac 1 2} \sum\limits_{k = 1}^{m} (-1)^k x_k\Big).
\end{equation}
\end{lm}

\begin{proof}
Note that inequality (\ref{K_ineq}) is equivalent to the same inequality for the function $M(s, x) = K(s, x) - s$.
Also note that $M$ decreases in $s$ since $M$ is convex in $s$ and
$$
M_s(s, x) = (1 + {\frac 1 {s^2}})^{{\frac {p(x)} 2} - 1} (1 + {\frac 1 {s^2}} - {\frac {p(x)}{ s^2}}) - 1 \rightarrow 0 \qquad {\rm as} \quad s \to \infty.
$$
Therefore,
\begin{multline*}
\sum\limits_{k = 1}^{m} M(s_k, x_k)
\ge M(s_1, x_1) + M(s_m, x_m)
\overset{a}{\ge} 2 M\big({\frac {s_1 + s_m} 2}, {\frac {x_m - x_1} 2}\big) \ge \\
\overset{b}{\ge} 2 M\Big({\frac 1 2} \sum\limits_{k = 1}^{m} s_k, {\frac {x_m - x_1} 2}\Big)
\overset{c}{\ge} 2 M\Big({\frac 1 2} \sum\limits_{k = 1}^{m} s_k, {\frac 1 2} \sum\limits_{k = 1}^{m} (-1)^k x_k\Big).
\end{multline*}
Here (a) follows from evenness of $M$ in $x$ and its convexity,
(b) follows from decreasing of $M$ in $s$,
(c) follows from increasing of $M$ in $x \ge 0$.
\end{proof}

\begin{lm}
\label{linear}
Suppose that $K(s, x)$ is even in $x$ and jointly convex in both arguments.
Then $\I(u^*) \le \I(u)$ for any piecewise linear $u \in \Wf[-1, 1]$.
\end{lm}

\begin{proof}
Denote by $L \subset [-1, 1]$ the set of nodes of $u$ (including the endpoints of the segment).
Take $U = u([- 1, 1]) \setminus u(L) $, i.e. $U$ is the image of $u$ without images of the nodes.
This set is a union of a finite number of disjoint intervals $U = \cup_j U_j$.
Note that for each $j$ the set $u^{-1}(U_j)$ is a union of an even number (say, $m_j$) of disjoint intervals.
Moreover, $u$ coincides with some linear function $y^j_k$, $k = 1, \dots, m_j$, on each interval.
Without loss of generality, we assume that the supports of $y^j_k$ are ordered by $k$ for any $j$,
that is $\sup dom(y^j_k) \le \inf dom(y^j_{k + 1})$.
Denote $b^j_k = |y^j_k{}'(x)|$ and
$$
Z = {\rm meas}\{ x \in (-1, 1) | u'(x) = 0 \} = {\rm meas}\{ x \in (-1, 1) | u^*{}'(x) = 0 \}.
$$
Then
\begin{multline*}
\I(u) - Z = \sum\limits_j \int\limits_{u^{-1}(U_j)} (1 + u'^2(x))^{\frac {p(x)} 2} dx
= \sum\limits_j \sum\limits_k \int\limits_{dom(y^j_k)} (1 + {y^j_k}'^2(x))^{\frac {p(x)} 2} dx =
\\ = \sum\limits_j \int\limits_{U_j} \sum\limits_k {\frac 1 {b^j_k}} (1 + b^j_k{}^2)^{\frac {p((y^j_k)^{-1}(y))} 2} dy
= \sum\limits_j \int\limits_{U_j} \sum\limits_k K\Big({\frac 1 {b^j_k}}, (y^j_k)^{-1}(y)\Big) dy.
\end{multline*}

Any point $y \in U$ has two symmetrical preimages with respect to function $u^*$,
therefore we can define $(u^*)^{-1}: U \to [0, 1]$.
Thus, for each $j$ we have
\begin{eqnarray*}
(u^*)^{-1} (y) & = & {\frac 1 2} \sum\limits_{k = 1}^{m_j} (-1)^k (y^j_k)^{-1}(y); \\
|((u^*)^{-1})'(y)| & = & {\frac 1 {|u^*{}'((u^*)^{-1}(y))|}} = {\frac 1 2} \sum\limits_{k = 1}^{m_j} {\frac 1 {b^j_k}} =: { \frac 1 {b_j^*} }.
\end{eqnarray*}
Since $u^*$ is even, we obtain
\begin{multline*}
\I(u^*) - Z = 2 \int\limits_{(u^*)^{-1}(U)} (1 + u^*{}'^2(x))^{p(x) \over 2} dx =
\\ = 2 \int\limits_U |((u^*)^{-1})'(y)| \cdot \big( 1 + { 1 \over ((u^*)^{-1})'(y)^2 } \big)^{p((u^*)^{-1}(y)) \over 2} dy =
\\ = 2 \sum\limits_j \int\limits_{U_j} {1 \over b_j^*} \big(1 + b_j^*{}^2 \big)^{{1 \over 2} p\big({1 \over 2} \sum\limits_{k = 1}^{m_j} (-1)^k (y^j_k)^{-1}(y)\big)} dy =
\\ = 2 \sum\limits_j \int\limits_{U_j} K\Big({1 \over 2} \sum\limits_{k = 1}^{m_j} {1 \over b^j_k}, {1 \over 2} \sum\limits_{k = 1}^{m_j} (-1)^k (y^j_k)^{-1}(y)\Big) dy.
\end{multline*}
To complete the proof, it is sufficient to show the following inequality for each $j$ and $y \in U$:
$$
\sum\limits_{k = 1}^{m_j} K\Big({1 \over b^j_k}, (y^j_k)^{-1}(y)\Big) \ge
2 K\Big({1 \over 2} \sum\limits_{k = 1}^{m_j} {1 \over b^j_k}, {1 \over 2} \sum\limits_{k = 1}^{m_j} (-1)^k (y^j_k)^{-1}(y)\Big).
$$
But this inequality is provided by Lemma \ref{quasiConv}.
\end{proof}

Now we can prove the result in general case.

\begin{thm}
\label{mainThm}
Suppose that $p$ is even and $K$ is jointly convex in both arguments.
Then for any $u \in \Wf[-1, 1]$ the inequality $\I(u^*) \le \I(u)$ holds.
\end{thm}

\begin{proof}
Since $p(x)$ is bounded, we can choose a sequence of piecewise constant functions $v_n$ converging to $u'$ in $L^{p(x)}$,
see \cite[Theorem~1.4.1]{Shar}). Denote by $u_n$ the primitive of $v_n$.
Changing $v_n$ if necessary we can assume without loss of generality that $u_n \ge 0$ and $u_n(\pm 1) = 0$.

By embedding $L^{p(x)}[-1, 1]\mapsto L^1[-1, 1]$ we have $u_n \to u$ in $\Wf[-1, 1]$.
Further, since $| \sqrt{ 1 + x^2 } - \sqrt{ 1 + y^2 } | \le | x - y |$ for all $x$ and $y$,
the relation $v_n \to u'$ in $L^{p(x)}$ implies $\I( u_n ) \to \I( u )$.

%Будем считать, что частичные интегралы функций $v_n$ на отрезках $[-1, x]$ неотрицательны,
%иначе в качестве $v$ возьмем производные модуля первообразных $v_n$.
%Это не ухудшит сходимость ({\color{red} неочевидно !!!!!!!!!!}) и обеспечит требуемое.
%Обозначим $w_n(x) = v_n(x) - \int\limits_{-1}^1 v_n(x) dx$ и $u_n \in \Wf[-1, 1]$ --- первообразные функций $w_n$.
%
%Покажем, что $w_n \to u'$ в $L^{p(x)}$:
%\begin{multline*}
%\norm{ w_n - u' }_{L^{p(x)}}
%\le \norm{ v_n - u' }_{L^{p(x)}} + \norm{ \int\limits_{-1}^1 v_n( x ) dx }_{L^{p(x)}} = \\
%= \norm{ v_n - u' }_{L^{p(x)}} + \norm{ \int\limits_{-1}^1 v_n( x ) - u'( x ) dx }_{L^{p(x)}}
%\le \norm{ v_n - u' }_{L^{p(x)}} + \norm{ \norm{ v_n - u' }_{L^1} }_{L^{p(x)}} \le \\
%\le \norm{ v_n - u' }_{L^{p(x)}} + \norm{ С \norm{ v_n - u' }_{L^{p(x)}} }_{L^{p(x)}} \to 0
%\end{multline*}
%В последнем неравенстве мы воспользовались вложением $L^{q(x)}$ в $L^{p(x)}$ при $q(x) \le p(x)$ для $q \equiv 1$.
%Тем самым, $u_n' \to u'$ в $L^{p(x)}$, а значит, $u_n \to u$ в $\Wf[-1, 1]$
%и $\sqrt{ 1 + u_n'^2 } \to \sqrt{ 1 + u'^2 }$ в $L^{p(x)}$, так как $\sqrt{ 1 + x^2 }$ --- сжимающее отображение.
%Последнее равносильно
%$$
%A( \sqrt{ 1 + u_n'^2 } - \sqrt{ 1 + u'^2 } ) \to 0, \text{\ где\ }
%\norm{ f }_{L^{p(x)}} = \inf\{ \lambda: A\Big( { f \over \lambda } \Big) \le 1 \},
%$$
%поскольку функция $A$ задает метрику, согласованную с нормой в $L^{p(x)}$.
%Отсюда следует, что $A( \sqrt{ 1 + u_n'^2 } ) \to A( \sqrt{ 1 + u'^2 } )$,
%что совпадает с утверждением $\I(u_n) \to \I(u)$.

By \cite[Theorem 1]{Br}, the convergence $u_n \to u$ in $\Wf[-1, 1]$ implies weak convergence $u_n^* \rightharpoondown u^*$ in $\Wf[-1, 1]$.
By the Tonelli theorem, the functional $\I$ is sequentially weakly lower semicontinuous (see, e.g., \cite[Theorem 3.5]{BGH}).
Thus,
$$
\I(u^*) \le \liminf \I(u_n^*) \le \lim \I(u_n) = \I(u).
$$
\end{proof}


\section{Multidimensional case}

%Описание области

\begin{thm}
Suppose $I(u^*) \le I(u)$ for any $u \in \Wf(\Omega)$.
Then $p$ is even and convex in $y$, and function
$$
\K_{x'}(y, d, c) = c \big(1 + {1 + d^2 \over c^2} \big)^{p(x', y) \over 2}
$$
is jointly convex.
\end{thm}

\begin{proof}
Consider two points $x_1 = (x_0', y_1), x_2 = (x_0', y_2)$ ($x_0' \in \omega, -1 < y_1 < y_2 < 1$).
We construct a function $u \in \Wf$ with nonzero gradient only in circumferences of $x_1, x_2$ and around the lateral boundary of a cylinder with axis $[x_1, x_2]$.
Namely,
$$
u(x) = \min\Big(
  \big( {y - y_1 \over c_1} + (x' - x_0') \cdot \B_1' \big)_+,
  \big( {y_2 - y \over c_2} + (x' - x_0') \cdot \B_2' \big)_+,
  \delta \big( w - |x' - x_0'| \big)_+,
  h
\Big).
$$
Here the parameters $c_1, c_2 > 0$ are inverse derivatives w.r.t. $y$ at the ``bases'' of the cylinder,
$\B_1', \B_2' \in \Real^{n - 1}$ are gradients w.r.t. $x'$ at the ``bases'',
$\delta > 0$ is the absolute value of the gradient at the lateral boundary, $w > 0$ is the cylinder radius,
while $h > 0$ is the maximal function value.

Given $y_1$, $y_2$, $c_1$, $c_2$, $\B_1'$, $\B_2'$, we choose $h$ and $\delta$ as functions of small parameter $w$.
%We suppose that $y_2 - y_1$ is much greater than $\delta$ and $h$ as well as the width of non-zero derivative area on the side of the cylinder $\varkappa = {h \over \delta}$.
We set $\varkappa\equiv {h \over \delta} := {w \over 2}$ ($\varkappa$ is the width of the lateral layer with non-zero gradient).
%Also we suppose that there is an area of positive measure in $\Omega$ where $u$ is constant and nonzero.
%In particular, $h = \delta \varkappa < \min( {1 \over c_1}, {1 \over c_2 } ) {y_2 - y_1 \over 2}$.

The left base of the set $S = \supp u$ is given by the system
$$
{y - y_1 \over c_1} + (x' - x_0') \cdot \B_1' = 0; \qquad |x' - x_0'| \le w.
$$
Thus, it is an $(n-1)$-dimensional prolate ellipsoid of revolution with the major semiaxis $\sqrt{w^2 + c_1^2 w^2 |\B_1'|^2}$ and radius $w$.
Denote $A_1$ the set of points with gradient magnitude of $\frac{1}{c_1}$.
Suppose $\B_1' = 0$.
Then $A_1$ is trapezium-like body with $n - 1$-balls of radia $w$ and $\frac{w}{2}$ as bases and measuring $h c_1$ in height.
Hence
$$
|A_1| = \frac{h c_1}{\frac{w}{2}} \int\limits_{\frac{w}{2}}^w |B^{n - 1}_r| dr = C_1 \delta c_1 w^n.
$$
Hereinafter all constants $C$ are absolute.
Note, that removing $\B_1' = 0$ condition skews the $A_1$ trapezium.
As already mentioned, its base becomes larger by the factor of $\sqrt{w^2 + c_1^2 w^2 |\B_1'|^2}$.
But at the same time its height becomes smaller by the same factor.
This means that $|A_1|$ does not depend on $\B_1'$.
Similarly, $|A_2| = C_1 \delta c_1 w^n$.
Note, that applying symmetrization to $u$ makes base derivatives of $(\B_1', \frac{1}{c_1})$ and $(\B_2', \frac{1}{c_2})$
both become $(\frac{c_1 \B_1' + c_2 \B_2'}{c_1 + c_2}, \frac{2}{c_1 + c_2})$.
Hence $A_1$ and $A_2$ become $A_1'$ and $A_2'$ with $|A_1'| = |A_2'| = C_1 \delta \frac{c_1 + c_2}{2} w^n$.

Denote $A_\delta$ the set of points with gradient magnitude of $\delta$.
Then
$$
|A_\delta|
\le (y_2 - y_1 + c_1 w |\B_1'| + c_2 w |\B_2'| ) \int\limits_{\frac{w}{2}}^w |S^{n - 2}_r| dr
\le (y_2 - y_1) C w^{n - 1}.
$$
%Same holds for $A_\delta'$ defined similarly for $u^*$.

Also denote $A_{const}$ and $A_{const}'$ the sets of points in $\supp u$ and $\supp u^*$ with zero gradient.
They have equal measure as well as $|\supp u| = |\supp u^*|$.

Setting $\frac{1}{c_1} = \frac{1}{c_2} = w^2$, $\delta = w^4$, $\B_1' = \B_2' = 0$ and $w \to 0$, the theorem assumptions imply
\begin{multline*}
0 \le (I(u) - |\supp u|) - (I(u^*) - |\supp u^*|)
\\ \le \big( (1 + w^4)^{\frac{p(\bar{x}_1)}{2}} - 1 \big) |A_1| + \big( (1 + w^4)^{\frac{p(\bar{x}_2)}{2}} - 1 \big) |A_2|
+ \big( (1 + w^8)^{\frac{P}{2}} - 1 \big) |A_\delta|
\\ - \big( (1 + w^4)^{\frac{p(\hat{x}_1)}{2}} - 1 + (1 + w^4)^{\frac{p(\hat{x}_1)}{2}} - 1 \big) |A_1'|
\\ \le w^4 \big(\frac{p(\bar{x}_1)}{2} + \frac{p(\bar{x}_2)}{2} + o(1)\big) C_1 w^{n + 2}
+ w^8 \big( \frac{P}{2} + o(1) \big) (y_2 - y_1) C w^{n - 1}
\\ - w^4 \big(\frac{p(\hat{x}_1)}{2} + \frac{p(\hat{x}_2)}{2} + o(1)\big) C_1 w^{n + 2}
\end{multline*}
Here $P = \max p(x', y)$, $\bar{x}_1 \in A_1$, $\bar{x}_2 \in A_2$, $\hat{x}_1 \in A_1'$, $\hat{x}_2 \in A_2'$.
This leads to an inequality $0 \le p(x_0', y_1) + p(x_0', y_2) - p(x_0', \frac{y_1 - y_2}{2}) - p(x_0', \frac{y_2 - y_1}{2})$.
Applying Proposition \ref{convProp} we prove that $p$ is convex and even in $y$.

Now take arbitrary $c_1, c_2, \B_1', \B_2'$, $\delta = w^2$ and set $w \to 0$. Then
\begin{multline*}
0 \le (I(u) - |A_{const}| - |A_\delta|) - (I(u^*) - |A_{const}'| - |A_\delta'|)
\\ \le ( 1 + \frac{1}{c_1^2} + |\B_1'|^2 )^{\frac{p(\bar{x}_1)}{2}} |A_1| + (1 + \frac{1}{c_2^2} + |\B_2'|^2 )^{\frac{p(\bar{x}_2)}{2}} |A_2|
+ \big( (1 + w^4)^{\frac{P}{2}} - 1 \big) |A_\delta|
\\ - \Big( \big(1 + \big( \frac{2}{c_1 + c_2} \big)^2 + \big( \frac{c_1 \B_1' + c_2 \B_2'}{c_1 + c_2} \big)^2 \big)^{\frac{p(\hat{x}_1)}{2}} +
\big(1 + \big( \frac{2}{c_1 + c_2} \big)^2 + \big( \frac{c_1 \B_1' + c_2 \B_2'}{c_1 + c_2} \big)^2 \big)^{\frac{p(\hat{x}_1)}{2}} \Big) |A_1'|
\\ \le \big( \K_{\bar{x}_1'}(\bar{y}_1, c_1 \B_1', c_1) + \K_{\bar{x}_2'}(\bar{y}_2, c_2 \B_2', c_2) \big) C_1 w^{n + 2}
+ (\frac{P}{2} + o(1)) C (y_2 - y_1) w^{n + 3}
\\ -\big( \K_{\hat{x}_1'}(\hat{y}_1, \frac{c_1 \B_1' + c_2 \B_2'}{2}, \frac{c_1 + c_2}{2}) + \K_{\hat{x}_2'}(\hat{y}_2, \frac{c_1 \B_1' + c_2 \B_2'}{2}, \frac{c_1 + c_2}{2}) \big) C_1 w^{n + 2}.
\end{multline*}
Here $P$, $\bar{x}_1 = (\bar{x}_1', \bar{y}_1)$, $\bar{x}_2 = (\bar{x}_2', \bar{y}_2)$, $\hat{x}_1 = (\hat{x}_1', \hat{y}_1)$, $\hat{x}_2 = (\hat{x}_2', \hat{y}_2)$ are as earlier.
Passing to the limit $w \to 0$, we obtain
\begin{multline*}
\K_{x_0'}(y_1, c_1 \B_1', c_1) + \K_{x_0'}(y_2, c_2 \B_2', c_2)
\\ \ge \K_{x_0'}\big(\frac{y_1 - y_2}{2}, \frac{c_1 \B_1' + c_2 \B_2'}{2}, \frac{c_1 + c_2}{2}\big) + \K_{x_0'}\big(\frac{y_2 - y_1}{2}, \frac{c_1 \B_1' + c_2 \B_2'}{2}, \frac{c_1 + c_2}{2}\big),
\end{multline*}
which implies $\K_{x_0'}$ is convex since it is even in $y$.

\end{proof}

\end{document}